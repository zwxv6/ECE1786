{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBJxUpnQ5zWE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f5cc94-21b2-4aa9-eef6-2956a24dcc6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.2/20.2 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.4/298.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 3.3.1\n",
        "# The first time you run this will download a 862MB size file to .vector_cache/glove.6B.zip\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\",dim=100) # embedding size = 100"
      ],
      "metadata": {
        "id": "NdmBz64R7ga4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba16a9c3-f2d6-437d-f492-bfcc07f0787a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.38MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:25<00:00, 15472.28it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class baselineModel(torch.nn.Module):\n",
        "  def __init__(self, vocab, embedding_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = torch.nn.Embedding.from_pretrained(vocab.vectors)\n",
        "    self.out = torch.nn.Linear(embedding_size, 1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    e = self.embedding(x)\n",
        "    average = torch.mean(e, 0, True)\n",
        "    logits = self.out(average)\n",
        "    return logits.reshape([-1])"
      ],
      "metadata": {
        "id": "dVLqW5peCrRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(torch.nn.Module):\n",
        "  def __init__(self, vocab,k1,k2,n1,n2):\n",
        "    super().__init__()\n",
        "    self.k1 = (k1, 100)\n",
        "    self.k2 = (k2, 100)\n",
        "    self.n1 = n1\n",
        "    self.n2 = n2\n",
        "    self.probabilityFunction = torch.nn.Sigmoid()\n",
        "\n",
        "    self.embedding = torch.nn.Embedding.from_pretrained(vocab.vectors)\n",
        "\n",
        "    self.conv1 = torch.nn.Conv2d(in_channels=1, out_channels=self.n1, kernel_size=self.k1, bias=False)\n",
        "    self.bn1 = torch.nn.BatchNorm2d(self.n1)\n",
        "    self.maxpool1 = torch.nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
        "\n",
        "    self.conv2 = torch.nn.Conv2d(in_channels=1, out_channels=self.n2, kernel_size=self.k2, bias=False)\n",
        "    self.bn2 = torch.nn.BatchNorm2d(self.n2)\n",
        "    self.maxpool2 = torch.nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
        "\n",
        "    self.out = torch.nn.Linear(self.n1+self.n2, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    e = self.embedding(x)\n",
        "    input = torch.transpose(e, 0, 1).unsqueeze(1)\n",
        "    x1 = self.conv1(input)\n",
        "    x1 = F.relu(x1)\n",
        "    x1 = self.bn1(x1)\n",
        "    x1 = self.maxpool1(x1)\n",
        "\n",
        "    x2 = self.conv2(input)\n",
        "    x2 = F.relu(x2)\n",
        "    x2 = self.bn2(x2)\n",
        "    x2 = self.maxpool2(x2)\n",
        "\n",
        "    concatenate = torch.cat((x1, x2), dim=1)\n",
        "    output = self.out(concatenate.squeeze())\n",
        "    logits = self.probabilityFunction(output)\n",
        "\n",
        "    return logits.reshape([-1])"
      ],
      "metadata": {
        "id": "ZlqL7kvVFr5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def runModel(sentence,type = 'Baseline Model'):\n",
        "  probabilityFunction = torch.nn.Sigmoid()\n",
        "\n",
        "  if type == 'Baseline Model':\n",
        "    checkpoint = torch.load('baselineModel.pt')\n",
        "    model = baselineModel(glove,glove.vectors.shape[1])\n",
        "    model.load_state_dict(checkpoint)\n",
        "    model.eval()\n",
        "\n",
        "    tokens = sentence.split()\n",
        "    token_ints = [glove.stoi.get(tok, len(glove.stoi)-1) for tok in tokens]\n",
        "    token_tensor = torch.LongTensor(token_ints).view(-1,1)\n",
        "\n",
        "    logit = model(x=token_tensor)\n",
        "    probability = probabilityFunction(logit)\n",
        "    probability = torch.maximum(probability, torch.tensor([1e-5]))\n",
        "    probability = torch.minimum(probability, torch.tensor([0.99999]))\n",
        "    Y_pred = torch.round(probability)\n",
        "\n",
        "  elif type == 'CNN Model':\n",
        "    checkpoint = torch.load('CNNModel.pt')\n",
        "    model = CNNModel(glove,k1=2,k2=4,n1=20,n2=20)\n",
        "    model.load_state_dict(checkpoint)\n",
        "    model.eval()\n",
        "\n",
        "    tokens = sentence.split()\n",
        "    token_ints = [glove.stoi.get(tok, len(glove.stoi)-1) for tok in tokens]\n",
        "    token_tensor = torch.LongTensor(token_ints).view(-1,1)\n",
        "\n",
        "    mark = 0\n",
        "    if len(token_tensor) < 4:\n",
        "      temp = []\n",
        "      for i in range(len(token_tensor)):\n",
        "        temp.append([int(token_tensor[i].detach().numpy())])\n",
        "      while(mark == 0):\n",
        "        if len(temp) < 4:\n",
        "          temp.append([1])\n",
        "        else:\n",
        "          mark = 1\n",
        "      token_tensor = torch.tensor(temp)\n",
        "    logit = model(x=token_tensor)\n",
        "    probability = probabilityFunction(logit)\n",
        "    probability = torch.maximum(probability, torch.tensor([1e-5]))\n",
        "    probability = torch.minimum(probability, torch.tensor([0.99999]))\n",
        "    Y_pred = torch.round(probability)\n",
        "\n",
        "  if Y_pred == 1:\n",
        "    result = 'Subjective'\n",
        "  elif Y_pred == 0:\n",
        "    result = 'Objective'\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "ISSxBbra7j5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runModel('moon','CNN Model')"
      ],
      "metadata": {
        "id": "qmBRRdvg_aHN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b638e0b8-93df-44fb-db82-e4171df02d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Objective'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runModel('hatred','CNN Model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CdYYRrzyOCrx",
        "outputId": "64fe4a00-bf80-4ae6-fdb8-8aac9def13b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Subjective'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# please upload baselineModel.pt and CNNModel.pt into colab\n",
        "gr.Interface(fn=runModel,\n",
        "      inputs=[\"text\",\n",
        "      gr.Radio(['Baseline Model', 'CNN Model'])\n",
        "      ],\n",
        "      outputs=\"text\").launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "A1ENI1yj_HK-",
        "outputId": "3015ede3-a1f4-4a8d-8fc1-1642e06d0597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://5f01d31881ccb39456.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5f01d31881ccb39456.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}