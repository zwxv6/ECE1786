{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb2U4SkyZgiG"
      },
      "source": [
        "# Assignment 3 Top-Level Code/Notebook\n",
        "### Training a language model base on Karpathy's minGPT codebase\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hUaGyDH8ZgiH",
        "outputId": "1d31963a-d435-47bb-8828-53485bc26195"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nimport nltk\\nnltk.download('punkt')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# The code below is needed for using Google Colab, so un comment this if that is what you're using\n",
        "\"\"\"\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaB9hGCpf22J",
        "outputId": "54aac237-695a-4769-9768-c86c6245ba99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZSM804f4ZgiI",
        "outputId": "c46a2049-ab06-4977-8158-49228d6bb042"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n%cd /content/drive/MyDrive/Colab\\\\ Notebooks/\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# The code below is also needed for using Google Colab\n",
        "# BEFORE executing this, you must place the mingpt folder supplied in the assignment\n",
        "# your google drive, within the folder \"Colab Notebooks\"\n",
        "#\n",
        "# It mounts and changes into the folder that contains mingpt, which you must upload to google drive\n",
        "# So un-comment it if you've uploaded mingpt to your google drive, into the  \"Colab Notebooks\" folder\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ar-QW_Qe-Xg",
        "outputId": "8bf9be81-ca04-47c7-a8fe-c86198eec255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJWIsQDkfYza",
        "outputId": "daf95cf4-e836-439f-96d1-0f18bb48c543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41LjnwE3ZgiI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from mingpt.bpe import BPETokenizer\n",
        "from mingpt.utils import set_seed\n",
        "set_seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rFSaKgZZgiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f881e9-51e0-43a1-fdc1-e5a135516f2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json to /root/.cache/mingpt/encoder.json\n",
            "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe to /root/.cache/mingpt/vocab.bpe\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Prepare the dataset to train the Language Model (LM)\n",
        "This implementation splits the sentences and so doesn't create training\n",
        "examples that cross sentences.\n",
        "\n",
        "This code is set so that it uses one of two possible datasets, which were also used in Assignment 1:\n",
        "SmallSimpleCorpus.txt or LargerCorpus.txt\n",
        "\n",
        "Arguments:\n",
        "            ds_choice: str. \"small\" or \"large\". (i.e. selects which of the two datasets)\n",
        "            split: str. \"train\" or \"test\".\n",
        "            truncation: int. If -1: no truncation on sentences. Otherwise: truncate to this specific length.\n",
        "\"\"\"\n",
        "\n",
        "class LanguageModelingDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ds_choice=\"small\", split=\"train\", truncation=-1):\n",
        "\n",
        "        base_path = \"./\"\n",
        "        fn = {\"small\": \"SmallSimpleCorpus.txt\", \"large\": \"LargerCorpus.txt\"}\n",
        "        self.ds_choice = ds_choice\n",
        "        self.truncation = truncation  # int. If -1, then\n",
        "        text = Path(base_path, fn[ds_choice]).read_text()\n",
        "        if ds_choice == \"large\":\n",
        "            # Remove the newline char in the middle of sentences\n",
        "            # The \"paragraph splitting\" newlines appear to be \\n\\n -- remove the duplications there\n",
        "            text = text.replace(\"\\n\\n\", \"$$^^$$\").replace(\"\\n\", \" \").replace(\"$$^^$$\", \"\\n\")\n",
        "        sentences = sent_tokenize(text)\n",
        "\n",
        "        # Train / test split\n",
        "        train, val = train_test_split(sentences, test_size=0.2, shuffle=False)\n",
        "        if split == \"train\":\n",
        "            raw_data = train\n",
        "        else:\n",
        "            raw_data = val\n",
        "\n",
        "        # Tokenize\n",
        "        self.tokenizer = BPETokenizer()\n",
        "        self.data = []  # List of 1-d pytorch tensor\n",
        "        for sent in raw_data:\n",
        "            tokenized = self.tokenizer(sent).view(-1)  # pytorch tensor\n",
        "            if truncation >= 0:\n",
        "                self.data.append(tokenized[:truncation])\n",
        "            else:\n",
        "                self.data.append(tokenized)\n",
        "\n",
        "        # Count some items\n",
        "        self.max_sentence_length = np.max([len(d) for d in self.data])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        \"\"\"\n",
        "        We have to set this to the max vocab size (i.e., that decided by the BPE tokenizer),\n",
        "        but actually, only a small number of vocab is used, especially for the small text.\n",
        "        \"\"\"\n",
        "        return 50257\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        The output should be a tuple x and y, both as pytorch tensors.\n",
        "        Please refer to the `run()` method in the mingpt/trainer.py script for\n",
        "        how the x and y are going to be used.\n",
        "        \"\"\"\n",
        "        x = self.data[idx][:-1]\n",
        "        y = self.data[idx][1:]\n",
        "        return (x, y)\n",
        "\n",
        "    def get_block_size(self):\n",
        "        \"\"\"\n",
        "        block_size is the size at which lines are truncated to ensure they are equal-length.\n",
        "        \"\"\"\n",
        "        return self.max_sentence_length\n",
        "\n",
        "# Instantiate the Training Dataset\n",
        "#train_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"train\")  # use this for the short corpus\n",
        "train_dataset = LanguageModelingDataset(ds_choice=\"large\", split=\"train\", truncation=512) #use this for long\n",
        "\n",
        "# Instantiate a Validation Dataset (this is only really needed for the fine-tune task, not the LM task)\n",
        "#val_dataset = LanguageModelingDataset(ds_choice=\"small\", split=\"validation\")\n",
        "val_dataset = LanguageModelingDataset(ds_choice=\"large\", split=\"validation\", truncation=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfRUKOM5ZgiI"
      },
      "outputs": [],
      "source": [
        "def lm_collate_fn(batch, device):\n",
        "    x = [item[0] for item in batch]  # List (len B) of varying lengths\n",
        "    y = [item[1] for item in batch]  # List (len B) of the same lengths as x\n",
        "    maxlen = max([len(s) for s in x])\n",
        "\n",
        "    padded_x, padded_y = [], []\n",
        "    for sx, sy in zip(x, y):\n",
        "        padded_x.append(torch.cat([sx, torch.ones(maxlen - len(sx))]))\n",
        "        padded_y.append(torch.cat([sy, torch.ones(maxlen - len(sy))]))\n",
        "    return torch.stack(padded_x).long().to(device), torch.stack(padded_y).long().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J2TxrojZgiI",
        "outputId": "a8c82a04-c160-4e13-a885-cbc5dd75cbcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([  40, 6437,  262, 3290]) tensor([6437,  262, 3290,   13])\n",
            "X:  I rub the dog\n",
            "Y:   rub the dog.\n"
          ]
        }
      ],
      "source": [
        "# Print out an example of the data - this is processed more once it reaches lm_collate_fn (above)\n",
        "x,y = train_dataset[5]\n",
        "print(x, y)\n",
        "print(\"X: \",train_dataset.tokenizer.decode(x))\n",
        "print(\"Y: \",train_dataset.tokenizer.decode(y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.tensor([0, 10, 3, 0], dtype=torch.float)\n",
        "torch.multinomial(weights, num_samples=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs0H9pmjuh24",
        "outputId": "6a2cc60d-85b7-4c99-a26b-e18043181d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JwqqJQZZgiJ",
        "outputId": "663551a1-10bc-4eec-b488-44179d9a4e2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 2.52M\n"
          ]
        }
      ],
      "source": [
        "from mingpt.model import GPT\n",
        "\n",
        "model_config = GPT.get_default_config()\n",
        "model_config.model_type = 'gpt-nano'\n",
        "model_config.vocab_size = train_dataset.get_vocab_size()\n",
        "model_config.block_size = train_dataset.get_block_size()\n",
        "model_config.n_classification_class = 2\n",
        "model = GPT(model_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKIuwtc5ZgiJ",
        "outputId": "517eb8bf-0050-4082-c1f8-d94f1da750e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on device cuda\n"
          ]
        }
      ],
      "source": [
        "# Create a Trainer object and set the core hyper-parameters\n",
        "from mingpt.trainer import Trainer\n",
        "\n",
        "train_config = Trainer.get_default_config()\n",
        "train_config.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n",
        "train_config.max_iters = 100000  # For small corpus: 3000 iterations is plenty. For large corpus: 100000 iterations is needed\n",
        "train_config.num_workers = 0\n",
        "train_config.batch_size = 16    # For small corpus, batch size of 4 is fine.  For large corpus use 16\n",
        "trainer = Trainer(train_config, model, train_dataset, val_dataset, collate_fn=lm_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uvM-lrLZgiJ",
        "outputId": "47cb16fe-48cf-40cf-af90-d1b626e96088",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter_dt 0.00ms; iter 0: train loss 1.44767\n",
            "iter_dt 21.43ms; iter 100: train loss 0.52188\n",
            "iter_dt 27.08ms; iter 200: train loss 0.43902\n",
            "iter_dt 29.23ms; iter 300: train loss 0.93636\n",
            "iter_dt 20.58ms; iter 400: train loss 0.50063\n",
            "iter_dt 22.77ms; iter 500: train loss 1.00063\n",
            "iter_dt 18.90ms; iter 600: train loss 0.79821\n",
            "iter_dt 31.31ms; iter 700: train loss 0.60460\n",
            "iter_dt 17.54ms; iter 800: train loss 0.95457\n",
            "iter_dt 20.28ms; iter 900: train loss 1.20016\n",
            "iter_dt 17.60ms; iter 1000: train loss 1.05357\n",
            "iter_dt 24.21ms; iter 1100: train loss 0.57932\n",
            "iter_dt 16.88ms; iter 1200: train loss 1.25023\n",
            "iter_dt 19.54ms; iter 1300: train loss 0.64882\n",
            "iter_dt 160.96ms; iter 1400: train loss 0.26326\n",
            "iter_dt 26.06ms; iter 1500: train loss 0.31194\n",
            "iter_dt 25.53ms; iter 1600: train loss 0.83218\n",
            "iter_dt 15.57ms; iter 1700: train loss 0.69789\n",
            "iter_dt 17.50ms; iter 1800: train loss 1.25245\n",
            "iter_dt 22.42ms; iter 1900: train loss 0.89088\n",
            "iter_dt 32.73ms; iter 2000: train loss 0.85548\n",
            "iter_dt 24.60ms; iter 2100: train loss 0.84355\n",
            "iter_dt 25.27ms; iter 2200: train loss 0.43247\n",
            "iter_dt 28.32ms; iter 2300: train loss 0.90886\n",
            "iter_dt 27.37ms; iter 2400: train loss 1.01319\n",
            "iter_dt 25.49ms; iter 2500: train loss 1.59785\n",
            "iter_dt 15.58ms; iter 2600: train loss 0.76715\n",
            "iter_dt 22.72ms; iter 2700: train loss 0.95363\n",
            "iter_dt 23.13ms; iter 2800: train loss 1.11836\n",
            "iter_dt 145.59ms; iter 2900: train loss 0.80714\n",
            "iter_dt 40.78ms; iter 3000: train loss 0.64981\n",
            "iter_dt 22.54ms; iter 3100: train loss 1.44625\n",
            "iter_dt 22.60ms; iter 3200: train loss 0.52885\n",
            "iter_dt 20.96ms; iter 3300: train loss 1.06463\n",
            "iter_dt 23.97ms; iter 3400: train loss 0.97963\n",
            "iter_dt 15.17ms; iter 3500: train loss 0.47041\n",
            "iter_dt 22.60ms; iter 3600: train loss 0.40552\n",
            "iter_dt 34.72ms; iter 3700: train loss 0.77329\n",
            "iter_dt 42.33ms; iter 3800: train loss 0.64191\n",
            "iter_dt 41.77ms; iter 3900: train loss 0.66532\n",
            "iter_dt 17.45ms; iter 4000: train loss 0.22072\n",
            "iter_dt 30.13ms; iter 4100: train loss 0.69408\n",
            "iter_dt 19.46ms; iter 4200: train loss 0.78585\n",
            "iter_dt 22.84ms; iter 4300: train loss 0.25947\n",
            "iter_dt 32.71ms; iter 4400: train loss 0.45148\n",
            "iter_dt 26.58ms; iter 4500: train loss 1.12629\n",
            "iter_dt 38.24ms; iter 4600: train loss 0.91810\n",
            "iter_dt 18.66ms; iter 4700: train loss 0.55195\n",
            "iter_dt 99.24ms; iter 4800: train loss 1.12657\n",
            "iter_dt 22.15ms; iter 4900: train loss 0.44187\n",
            "iter_dt 21.47ms; iter 5000: train loss 0.56619\n",
            "iter_dt 18.95ms; iter 5100: train loss 0.42347\n",
            "iter_dt 23.56ms; iter 5200: train loss 0.83204\n",
            "iter_dt 35.01ms; iter 5300: train loss 0.66852\n",
            "iter_dt 25.80ms; iter 5400: train loss 0.65015\n",
            "iter_dt 15.72ms; iter 5500: train loss 0.84971\n",
            "iter_dt 17.72ms; iter 5600: train loss 0.90050\n",
            "iter_dt 22.40ms; iter 5700: train loss 1.01043\n",
            "iter_dt 20.11ms; iter 5800: train loss 0.88095\n",
            "iter_dt 18.68ms; iter 5900: train loss 0.52791\n",
            "iter_dt 19.46ms; iter 6000: train loss 0.59452\n",
            "iter_dt 144.90ms; iter 6100: train loss 1.81695\n",
            "iter_dt 64.93ms; iter 6200: train loss 1.18959\n",
            "iter_dt 29.62ms; iter 6300: train loss 1.33707\n",
            "iter_dt 34.42ms; iter 6400: train loss 1.18995\n",
            "iter_dt 26.09ms; iter 6500: train loss 0.68755\n",
            "iter_dt 23.98ms; iter 6600: train loss 1.05062\n",
            "iter_dt 17.70ms; iter 6700: train loss 0.47436\n",
            "iter_dt 18.26ms; iter 6800: train loss 0.83114\n",
            "iter_dt 26.42ms; iter 6900: train loss 0.35368\n",
            "iter_dt 32.88ms; iter 7000: train loss 0.37993\n",
            "iter_dt 23.61ms; iter 7100: train loss 1.17301\n",
            "iter_dt 18.04ms; iter 7200: train loss 0.63621\n",
            "iter_dt 21.38ms; iter 7300: train loss 0.75462\n",
            "iter_dt 27.37ms; iter 7400: train loss 0.55739\n",
            "iter_dt 30.15ms; iter 7500: train loss 1.38127\n",
            "iter_dt 33.53ms; iter 7600: train loss 1.12517\n",
            "iter_dt 23.63ms; iter 7700: train loss 0.92319\n",
            "iter_dt 20.39ms; iter 7800: train loss 0.82003\n",
            "iter_dt 21.01ms; iter 7900: train loss 0.95809\n",
            "iter_dt 15.69ms; iter 8000: train loss 0.91679\n",
            "iter_dt 24.78ms; iter 8100: train loss 1.37312\n",
            "iter_dt 15.83ms; iter 8200: train loss 0.84780\n",
            "iter_dt 23.21ms; iter 8300: train loss 1.07866\n",
            "iter_dt 35.74ms; iter 8400: train loss 0.73590\n",
            "iter_dt 18.59ms; iter 8500: train loss 0.90025\n",
            "iter_dt 17.49ms; iter 8600: train loss 0.46102\n",
            "iter_dt 23.18ms; iter 8700: train loss 0.86455\n",
            "iter_dt 24.42ms; iter 8800: train loss 1.25813\n",
            "iter_dt 26.87ms; iter 8900: train loss 0.54927\n",
            "iter_dt 23.47ms; iter 9000: train loss 1.00066\n",
            "iter_dt 29.69ms; iter 9100: train loss 0.67791\n",
            "iter_dt 15.40ms; iter 9200: train loss 0.70918\n",
            "iter_dt 19.42ms; iter 9300: train loss 0.82487\n",
            "iter_dt 25.32ms; iter 9400: train loss 0.58366\n",
            "iter_dt 17.33ms; iter 9500: train loss 0.59173\n",
            "iter_dt 15.74ms; iter 9600: train loss 1.00148\n",
            "iter_dt 20.33ms; iter 9700: train loss 0.95409\n",
            "iter_dt 28.52ms; iter 9800: train loss 1.05451\n",
            "iter_dt 17.22ms; iter 9900: train loss 0.98304\n",
            "iter_dt 26.71ms; iter 10000: train loss 0.49067\n",
            "iter_dt 21.05ms; iter 10100: train loss 0.51169\n",
            "iter_dt 29.82ms; iter 10200: train loss 1.05222\n",
            "iter_dt 57.74ms; iter 10300: train loss 0.66009\n",
            "iter_dt 20.08ms; iter 10400: train loss 0.48273\n",
            "iter_dt 24.58ms; iter 10500: train loss 0.79073\n",
            "iter_dt 21.78ms; iter 10600: train loss 0.90922\n",
            "iter_dt 24.92ms; iter 10700: train loss 0.67130\n",
            "iter_dt 22.92ms; iter 10800: train loss 0.53989\n",
            "iter_dt 25.98ms; iter 10900: train loss 0.59792\n",
            "iter_dt 20.52ms; iter 11000: train loss 0.67711\n",
            "iter_dt 17.08ms; iter 11100: train loss 0.53493\n",
            "iter_dt 19.34ms; iter 11200: train loss 0.58131\n",
            "iter_dt 21.52ms; iter 11300: train loss 0.71366\n",
            "iter_dt 31.47ms; iter 11400: train loss 1.22476\n",
            "iter_dt 17.86ms; iter 11500: train loss 0.73093\n",
            "iter_dt 19.30ms; iter 11600: train loss 0.69395\n",
            "iter_dt 23.56ms; iter 11700: train loss 0.44113\n",
            "iter_dt 41.83ms; iter 11800: train loss 0.68304\n",
            "iter_dt 18.11ms; iter 11900: train loss 0.73335\n",
            "iter_dt 33.12ms; iter 12000: train loss 0.71642\n",
            "iter_dt 20.93ms; iter 12100: train loss 0.47656\n",
            "iter_dt 27.34ms; iter 12200: train loss 0.43510\n",
            "iter_dt 20.79ms; iter 12300: train loss 0.59481\n",
            "iter_dt 19.32ms; iter 12400: train loss 0.88319\n",
            "iter_dt 32.48ms; iter 12500: train loss 0.80727\n",
            "iter_dt 15.59ms; iter 12600: train loss 0.77443\n",
            "iter_dt 21.18ms; iter 12700: train loss 0.45139\n",
            "iter_dt 22.47ms; iter 12800: train loss 0.36592\n",
            "iter_dt 21.07ms; iter 12900: train loss 1.10348\n",
            "iter_dt 22.51ms; iter 13000: train loss 0.67559\n",
            "iter_dt 26.41ms; iter 13100: train loss 0.25261\n",
            "iter_dt 17.69ms; iter 13200: train loss 0.66039\n",
            "iter_dt 16.66ms; iter 13300: train loss 1.19873\n",
            "iter_dt 100.69ms; iter 13400: train loss 0.82815\n",
            "iter_dt 41.43ms; iter 13500: train loss 0.65459\n",
            "iter_dt 26.30ms; iter 13600: train loss 0.90205\n",
            "iter_dt 21.68ms; iter 13700: train loss 0.85316\n",
            "iter_dt 33.24ms; iter 13800: train loss 0.43267\n",
            "iter_dt 19.32ms; iter 13900: train loss 0.33190\n",
            "iter_dt 19.00ms; iter 14000: train loss 0.84241\n",
            "iter_dt 11.84ms; iter 14100: train loss 0.94858\n",
            "iter_dt 19.13ms; iter 14200: train loss 0.89692\n",
            "iter_dt 25.86ms; iter 14300: train loss 0.75585\n",
            "iter_dt 23.87ms; iter 14400: train loss 0.97893\n",
            "iter_dt 17.52ms; iter 14500: train loss 0.60312\n",
            "iter_dt 32.02ms; iter 14600: train loss 0.61240\n",
            "iter_dt 28.36ms; iter 14700: train loss 0.34531\n",
            "iter_dt 31.16ms; iter 14800: train loss 0.48506\n",
            "iter_dt 16.10ms; iter 14900: train loss 0.59205\n",
            "iter_dt 30.33ms; iter 15000: train loss 0.85534\n",
            "iter_dt 17.21ms; iter 15100: train loss 0.48363\n",
            "iter_dt 33.87ms; iter 15200: train loss 0.71448\n",
            "iter_dt 18.61ms; iter 15300: train loss 0.59382\n",
            "iter_dt 25.41ms; iter 15400: train loss 0.85947\n",
            "iter_dt 25.63ms; iter 15500: train loss 0.62117\n",
            "iter_dt 99.03ms; iter 15600: train loss 0.54861\n",
            "iter_dt 32.66ms; iter 15700: train loss 0.95892\n",
            "iter_dt 20.68ms; iter 15800: train loss 0.92673\n",
            "iter_dt 31.63ms; iter 15900: train loss 0.92258\n",
            "iter_dt 27.96ms; iter 16000: train loss 0.46720\n",
            "iter_dt 19.88ms; iter 16100: train loss 0.78921\n",
            "iter_dt 40.12ms; iter 16200: train loss 0.48844\n",
            "iter_dt 23.34ms; iter 16300: train loss 0.68128\n",
            "iter_dt 19.43ms; iter 16400: train loss 0.88432\n",
            "iter_dt 15.93ms; iter 16500: train loss 0.52110\n",
            "iter_dt 22.35ms; iter 16600: train loss 0.44401\n",
            "iter_dt 22.82ms; iter 16700: train loss 0.40992\n",
            "iter_dt 33.08ms; iter 16800: train loss 1.28975\n",
            "iter_dt 34.42ms; iter 16900: train loss 0.31558\n",
            "iter_dt 15.32ms; iter 17000: train loss 0.82434\n",
            "iter_dt 15.48ms; iter 17100: train loss 0.29663\n",
            "iter_dt 24.53ms; iter 17200: train loss 0.54401\n",
            "iter_dt 39.49ms; iter 17300: train loss 0.67135\n",
            "iter_dt 20.10ms; iter 17400: train loss 1.03686\n",
            "iter_dt 27.78ms; iter 17500: train loss 1.05627\n",
            "iter_dt 22.44ms; iter 17600: train loss 0.65752\n",
            "iter_dt 32.22ms; iter 17700: train loss 0.49455\n",
            "iter_dt 29.28ms; iter 17800: train loss 0.87786\n",
            "iter_dt 34.11ms; iter 17900: train loss 0.79686\n",
            "iter_dt 13.79ms; iter 18000: train loss 0.32848\n",
            "iter_dt 144.29ms; iter 18100: train loss 0.91042\n",
            "iter_dt 25.45ms; iter 18200: train loss 0.51379\n",
            "iter_dt 16.06ms; iter 18300: train loss 0.26475\n",
            "iter_dt 21.26ms; iter 18400: train loss 0.97561\n",
            "iter_dt 25.12ms; iter 18500: train loss 0.61318\n",
            "iter_dt 26.61ms; iter 18600: train loss 0.71947\n",
            "iter_dt 19.36ms; iter 18700: train loss 0.27784\n",
            "iter_dt 22.57ms; iter 18800: train loss 0.83431\n",
            "iter_dt 29.62ms; iter 18900: train loss 0.45732\n",
            "iter_dt 20.88ms; iter 19000: train loss 1.14403\n",
            "iter_dt 31.70ms; iter 19100: train loss 0.60697\n",
            "iter_dt 20.65ms; iter 19200: train loss 0.91356\n",
            "iter_dt 19.22ms; iter 19300: train loss 0.94105\n",
            "iter_dt 26.25ms; iter 19400: train loss 0.80291\n",
            "iter_dt 17.47ms; iter 19500: train loss 0.54065\n",
            "iter_dt 26.19ms; iter 19600: train loss 0.99798\n",
            "iter_dt 23.31ms; iter 19700: train loss 0.81032\n",
            "iter_dt 20.75ms; iter 19800: train loss 1.02127\n",
            "iter_dt 20.78ms; iter 19900: train loss 0.70700\n",
            "iter_dt 35.05ms; iter 20000: train loss 0.58347\n",
            "iter_dt 29.45ms; iter 20100: train loss 0.72636\n",
            "iter_dt 44.25ms; iter 20200: train loss 0.62753\n",
            "iter_dt 15.60ms; iter 20300: train loss 0.76474\n",
            "iter_dt 17.11ms; iter 20400: train loss 0.52347\n",
            "iter_dt 21.61ms; iter 20500: train loss 0.75471\n",
            "iter_dt 29.45ms; iter 20600: train loss 0.88780\n",
            "iter_dt 17.37ms; iter 20700: train loss 0.66365\n",
            "iter_dt 18.57ms; iter 20800: train loss 0.57527\n",
            "iter_dt 18.63ms; iter 20900: train loss 0.51015\n",
            "iter_dt 24.70ms; iter 21000: train loss 0.67670\n",
            "iter_dt 36.49ms; iter 21100: train loss 0.85685\n",
            "iter_dt 15.86ms; iter 21200: train loss 0.93886\n",
            "iter_dt 19.94ms; iter 21300: train loss 0.68727\n",
            "iter_dt 26.26ms; iter 21400: train loss 0.75551\n",
            "iter_dt 43.85ms; iter 21500: train loss 0.86059\n",
            "iter_dt 42.01ms; iter 21600: train loss 0.83369\n",
            "iter_dt 15.30ms; iter 21700: train loss 0.62771\n",
            "iter_dt 27.37ms; iter 21800: train loss 0.53476\n",
            "iter_dt 30.02ms; iter 21900: train loss 0.75086\n",
            "iter_dt 24.19ms; iter 22000: train loss 0.78812\n",
            "iter_dt 41.05ms; iter 22100: train loss 0.58894\n",
            "iter_dt 22.63ms; iter 22200: train loss 0.71716\n",
            "iter_dt 41.79ms; iter 22300: train loss 0.77187\n",
            "iter_dt 23.36ms; iter 22400: train loss 0.61161\n",
            "iter_dt 28.68ms; iter 22500: train loss 0.85167\n",
            "iter_dt 145.31ms; iter 22600: train loss 0.89656\n",
            "iter_dt 22.38ms; iter 22700: train loss 0.52532\n",
            "iter_dt 29.60ms; iter 22800: train loss 0.64337\n",
            "iter_dt 28.68ms; iter 22900: train loss 0.44262\n",
            "iter_dt 21.19ms; iter 23000: train loss 0.70885\n",
            "iter_dt 39.30ms; iter 23100: train loss 0.89148\n",
            "iter_dt 26.69ms; iter 23200: train loss 0.17378\n",
            "iter_dt 34.46ms; iter 23300: train loss 0.37986\n",
            "iter_dt 16.88ms; iter 23400: train loss 0.38567\n",
            "iter_dt 23.57ms; iter 23500: train loss 1.22387\n",
            "iter_dt 23.72ms; iter 23600: train loss 1.04288\n",
            "iter_dt 20.20ms; iter 23700: train loss 0.48139\n",
            "iter_dt 39.26ms; iter 23800: train loss 0.70911\n",
            "iter_dt 16.28ms; iter 23900: train loss 0.80269\n",
            "iter_dt 17.91ms; iter 24000: train loss 0.56776\n",
            "iter_dt 30.78ms; iter 24100: train loss 0.47120\n",
            "iter_dt 18.46ms; iter 24200: train loss 0.54910\n",
            "iter_dt 22.28ms; iter 24300: train loss 0.70150\n",
            "iter_dt 27.17ms; iter 24400: train loss 0.86742\n",
            "iter_dt 26.30ms; iter 24500: train loss 0.54869\n",
            "iter_dt 27.57ms; iter 24600: train loss 0.31776\n",
            "iter_dt 21.00ms; iter 24700: train loss 0.77070\n",
            "iter_dt 15.16ms; iter 24800: train loss 0.84465\n",
            "iter_dt 24.83ms; iter 24900: train loss 0.74964\n",
            "iter_dt 27.10ms; iter 25000: train loss 0.69717\n",
            "iter_dt 23.18ms; iter 25100: train loss 0.87149\n",
            "iter_dt 28.50ms; iter 25200: train loss 0.83966\n",
            "iter_dt 25.10ms; iter 25300: train loss 0.86549\n",
            "iter_dt 53.16ms; iter 25400: train loss 0.72922\n",
            "iter_dt 19.42ms; iter 25500: train loss 0.63984\n",
            "iter_dt 26.30ms; iter 25600: train loss 0.76851\n",
            "iter_dt 15.70ms; iter 25700: train loss 0.51234\n",
            "iter_dt 28.76ms; iter 25800: train loss 0.55875\n",
            "iter_dt 21.85ms; iter 25900: train loss 0.86185\n",
            "iter_dt 33.09ms; iter 26000: train loss 0.75960\n",
            "iter_dt 32.47ms; iter 26100: train loss 0.30271\n",
            "iter_dt 22.28ms; iter 26200: train loss 0.18092\n",
            "iter_dt 25.40ms; iter 26300: train loss 0.79036\n",
            "iter_dt 30.42ms; iter 26400: train loss 0.41530\n",
            "iter_dt 30.54ms; iter 26500: train loss 0.41615\n",
            "iter_dt 23.07ms; iter 26600: train loss 0.66642\n",
            "iter_dt 15.08ms; iter 26700: train loss 0.59824\n",
            "iter_dt 32.83ms; iter 26800: train loss 0.63535\n",
            "iter_dt 20.89ms; iter 26900: train loss 0.64574\n",
            "iter_dt 18.50ms; iter 27000: train loss 0.41353\n",
            "iter_dt 32.35ms; iter 27100: train loss 0.14745\n",
            "iter_dt 16.09ms; iter 27200: train loss 0.48906\n",
            "iter_dt 23.88ms; iter 27300: train loss 0.62856\n",
            "iter_dt 15.62ms; iter 27400: train loss 0.69223\n",
            "iter_dt 23.73ms; iter 27500: train loss 0.79801\n",
            "iter_dt 22.59ms; iter 27600: train loss 0.41929\n",
            "iter_dt 15.54ms; iter 27700: train loss 0.91773\n",
            "iter_dt 20.53ms; iter 27800: train loss 0.69430\n",
            "iter_dt 26.24ms; iter 27900: train loss 0.67932\n",
            "iter_dt 29.69ms; iter 28000: train loss 0.60873\n",
            "iter_dt 24.08ms; iter 28100: train loss 0.52134\n",
            "iter_dt 31.26ms; iter 28200: train loss 0.79318\n",
            "iter_dt 30.35ms; iter 28300: train loss 0.75600\n",
            "iter_dt 24.43ms; iter 28400: train loss 0.50963\n",
            "iter_dt 22.27ms; iter 28500: train loss 0.34989\n",
            "iter_dt 22.70ms; iter 28600: train loss 0.79657\n",
            "iter_dt 21.41ms; iter 28700: train loss 0.71416\n",
            "iter_dt 27.45ms; iter 28800: train loss 0.73232\n",
            "iter_dt 21.82ms; iter 28900: train loss 0.91888\n",
            "iter_dt 39.58ms; iter 29000: train loss 0.80335\n",
            "iter_dt 22.64ms; iter 29100: train loss 0.84855\n",
            "iter_dt 33.16ms; iter 29200: train loss 0.54304\n",
            "iter_dt 33.31ms; iter 29300: train loss 0.48424\n",
            "iter_dt 18.60ms; iter 29400: train loss 0.67768\n",
            "iter_dt 20.49ms; iter 29500: train loss 0.70670\n",
            "iter_dt 47.78ms; iter 29600: train loss 0.77180\n",
            "iter_dt 26.60ms; iter 29700: train loss 0.67590\n",
            "iter_dt 25.24ms; iter 29800: train loss 0.27045\n",
            "iter_dt 22.17ms; iter 29900: train loss 0.62850\n",
            "iter_dt 24.63ms; iter 30000: train loss 0.62610\n",
            "iter_dt 18.61ms; iter 30100: train loss 0.98532\n",
            "iter_dt 22.28ms; iter 30200: train loss 1.01755\n",
            "iter_dt 22.58ms; iter 30300: train loss 0.36523\n",
            "iter_dt 19.86ms; iter 30400: train loss 0.97253\n",
            "iter_dt 22.15ms; iter 30500: train loss 0.76591\n",
            "iter_dt 23.06ms; iter 30600: train loss 0.91863\n",
            "iter_dt 145.23ms; iter 30700: train loss 0.64384\n",
            "iter_dt 16.01ms; iter 30800: train loss 1.02377\n",
            "iter_dt 19.59ms; iter 30900: train loss 0.76027\n",
            "iter_dt 21.44ms; iter 31000: train loss 1.11273\n",
            "iter_dt 34.17ms; iter 31100: train loss 0.67848\n",
            "iter_dt 20.95ms; iter 31200: train loss 0.57530\n",
            "iter_dt 39.04ms; iter 31300: train loss 0.57014\n",
            "iter_dt 18.97ms; iter 31400: train loss 0.38309\n",
            "iter_dt 23.63ms; iter 31500: train loss 0.67188\n",
            "iter_dt 24.62ms; iter 31600: train loss 0.60616\n",
            "iter_dt 15.32ms; iter 31700: train loss 0.63307\n",
            "iter_dt 20.90ms; iter 31800: train loss 0.77436\n",
            "iter_dt 15.79ms; iter 31900: train loss 0.72245\n",
            "iter_dt 23.07ms; iter 32000: train loss 0.53883\n",
            "iter_dt 15.00ms; iter 32100: train loss 0.94303\n",
            "iter_dt 24.96ms; iter 32200: train loss 0.54452\n",
            "iter_dt 25.58ms; iter 32300: train loss 0.55230\n",
            "iter_dt 18.28ms; iter 32400: train loss 0.83425\n",
            "iter_dt 28.26ms; iter 32500: train loss 1.48785\n",
            "iter_dt 65.85ms; iter 32600: train loss 0.66554\n",
            "iter_dt 23.06ms; iter 32700: train loss 0.66155\n",
            "iter_dt 18.36ms; iter 32800: train loss 1.04708\n",
            "iter_dt 15.78ms; iter 32900: train loss 1.07284\n",
            "iter_dt 35.78ms; iter 33000: train loss 0.53591\n",
            "iter_dt 19.87ms; iter 33100: train loss 0.85358\n",
            "iter_dt 21.16ms; iter 33200: train loss 0.87623\n",
            "iter_dt 19.90ms; iter 33300: train loss 0.78970\n",
            "iter_dt 25.01ms; iter 33400: train loss 0.86620\n",
            "iter_dt 26.99ms; iter 33500: train loss 0.92666\n",
            "iter_dt 33.11ms; iter 33600: train loss 0.46749\n",
            "iter_dt 77.62ms; iter 33700: train loss 0.42169\n",
            "iter_dt 17.78ms; iter 33800: train loss 0.83511\n",
            "iter_dt 26.90ms; iter 33900: train loss 1.14535\n",
            "iter_dt 40.60ms; iter 34000: train loss 0.52081\n",
            "iter_dt 25.19ms; iter 34100: train loss 0.19059\n",
            "iter_dt 34.91ms; iter 34200: train loss 0.94082\n",
            "iter_dt 28.23ms; iter 34300: train loss 0.28354\n",
            "iter_dt 16.73ms; iter 34400: train loss 0.71350\n",
            "iter_dt 30.66ms; iter 34500: train loss 0.38471\n",
            "iter_dt 24.61ms; iter 34600: train loss 0.68223\n",
            "iter_dt 22.43ms; iter 34700: train loss 0.90307\n",
            "iter_dt 29.39ms; iter 34800: train loss 0.74076\n",
            "iter_dt 14.12ms; iter 34900: train loss 0.60503\n",
            "iter_dt 18.57ms; iter 35000: train loss 0.51663\n",
            "iter_dt 29.83ms; iter 35100: train loss 0.87594\n",
            "iter_dt 21.93ms; iter 35200: train loss 0.41441\n",
            "iter_dt 19.64ms; iter 35300: train loss 0.29188\n",
            "iter_dt 29.22ms; iter 35400: train loss 0.49613\n",
            "iter_dt 34.57ms; iter 35500: train loss 1.00381\n",
            "iter_dt 37.97ms; iter 35600: train loss 0.28290\n",
            "iter_dt 22.76ms; iter 35700: train loss 0.80292\n",
            "iter_dt 30.46ms; iter 35800: train loss 0.86156\n",
            "iter_dt 26.05ms; iter 35900: train loss 0.88620\n",
            "iter_dt 20.44ms; iter 36000: train loss 0.73597\n",
            "iter_dt 24.33ms; iter 36100: train loss 0.80202\n",
            "iter_dt 19.87ms; iter 36200: train loss 0.82504\n",
            "iter_dt 21.39ms; iter 36300: train loss 0.63087\n",
            "iter_dt 21.20ms; iter 36400: train loss 0.51081\n",
            "iter_dt 23.12ms; iter 36500: train loss 0.64251\n",
            "iter_dt 19.69ms; iter 36600: train loss 0.98837\n",
            "iter_dt 20.27ms; iter 36700: train loss 1.01538\n",
            "iter_dt 25.26ms; iter 36800: train loss 0.48733\n",
            "iter_dt 21.35ms; iter 36900: train loss 0.67303\n",
            "iter_dt 44.07ms; iter 37000: train loss 0.84539\n",
            "iter_dt 32.03ms; iter 37100: train loss 0.93481\n",
            "iter_dt 21.61ms; iter 37200: train loss 0.53300\n",
            "iter_dt 40.02ms; iter 37300: train loss 0.95149\n",
            "iter_dt 66.99ms; iter 37400: train loss 0.71441\n",
            "iter_dt 29.49ms; iter 37500: train loss 0.48733\n",
            "iter_dt 63.53ms; iter 37600: train loss 0.37138\n",
            "iter_dt 31.83ms; iter 37700: train loss 0.74036\n",
            "iter_dt 23.94ms; iter 37800: train loss 0.32792\n",
            "iter_dt 24.12ms; iter 37900: train loss 0.88495\n",
            "iter_dt 33.97ms; iter 38000: train loss 0.73582\n",
            "iter_dt 24.77ms; iter 38100: train loss 0.65611\n",
            "iter_dt 20.88ms; iter 38200: train loss 0.51461\n",
            "iter_dt 26.54ms; iter 38300: train loss 0.48056\n",
            "iter_dt 29.23ms; iter 38400: train loss 0.34179\n",
            "iter_dt 35.23ms; iter 38500: train loss 0.80033\n",
            "iter_dt 22.27ms; iter 38600: train loss 1.34015\n",
            "iter_dt 16.51ms; iter 38700: train loss 0.95674\n",
            "iter_dt 26.02ms; iter 38800: train loss 0.71778\n",
            "iter_dt 25.67ms; iter 38900: train loss 0.77365\n",
            "iter_dt 26.44ms; iter 39000: train loss 0.56013\n",
            "iter_dt 26.40ms; iter 39100: train loss 0.72024\n",
            "iter_dt 20.11ms; iter 39200: train loss 0.52931\n",
            "iter_dt 23.46ms; iter 39300: train loss 0.70144\n",
            "iter_dt 21.90ms; iter 39400: train loss 0.72586\n",
            "iter_dt 35.70ms; iter 39500: train loss 0.53194\n",
            "iter_dt 17.75ms; iter 39600: train loss 0.95637\n",
            "iter_dt 19.31ms; iter 39700: train loss 0.67434\n",
            "iter_dt 34.43ms; iter 39800: train loss 0.57458\n",
            "iter_dt 40.63ms; iter 39900: train loss 0.75430\n",
            "iter_dt 20.78ms; iter 40000: train loss 0.65364\n",
            "iter_dt 18.05ms; iter 40100: train loss 0.60362\n",
            "iter_dt 33.25ms; iter 40200: train loss 0.67003\n",
            "iter_dt 32.80ms; iter 40300: train loss 0.59352\n",
            "iter_dt 29.00ms; iter 40400: train loss 0.60119\n",
            "iter_dt 19.08ms; iter 40500: train loss 0.66552\n",
            "iter_dt 23.01ms; iter 40600: train loss 0.85610\n",
            "iter_dt 13.94ms; iter 40700: train loss 0.20946\n",
            "iter_dt 20.60ms; iter 40800: train loss 0.64783\n",
            "iter_dt 22.28ms; iter 40900: train loss 1.05091\n",
            "iter_dt 20.56ms; iter 41000: train loss 0.84320\n",
            "iter_dt 20.98ms; iter 41100: train loss 0.64550\n",
            "iter_dt 19.91ms; iter 41200: train loss 0.63884\n",
            "iter_dt 20.77ms; iter 41300: train loss 0.58909\n",
            "iter_dt 22.01ms; iter 41400: train loss 0.69219\n",
            "iter_dt 16.95ms; iter 41500: train loss 0.29262\n",
            "iter_dt 26.23ms; iter 41600: train loss 0.40774\n",
            "iter_dt 44.53ms; iter 41700: train loss 0.64241\n",
            "iter_dt 33.24ms; iter 41800: train loss 0.47393\n",
            "iter_dt 31.27ms; iter 41900: train loss 0.34085\n",
            "iter_dt 22.16ms; iter 42000: train loss 0.71223\n",
            "iter_dt 38.42ms; iter 42100: train loss 0.10360\n",
            "iter_dt 20.88ms; iter 42200: train loss 0.52404\n",
            "iter_dt 19.24ms; iter 42300: train loss 0.69409\n",
            "iter_dt 26.40ms; iter 42400: train loss 0.43627\n",
            "iter_dt 26.05ms; iter 42500: train loss 0.33578\n",
            "iter_dt 20.64ms; iter 42600: train loss 0.63260\n",
            "iter_dt 51.54ms; iter 42700: train loss 0.63282\n",
            "iter_dt 31.39ms; iter 42800: train loss 0.70838\n",
            "iter_dt 27.23ms; iter 42900: train loss 0.66886\n",
            "iter_dt 22.63ms; iter 43000: train loss 0.59785\n",
            "iter_dt 20.29ms; iter 43100: train loss 0.83289\n",
            "iter_dt 20.01ms; iter 43200: train loss 1.16061\n",
            "iter_dt 19.57ms; iter 43300: train loss 0.43732\n",
            "iter_dt 25.71ms; iter 43400: train loss 0.15311\n",
            "iter_dt 37.73ms; iter 43500: train loss 0.83547\n",
            "iter_dt 17.17ms; iter 43600: train loss 0.61805\n",
            "iter_dt 20.77ms; iter 43700: train loss 0.81498\n",
            "iter_dt 12.92ms; iter 43800: train loss 0.69585\n",
            "iter_dt 21.16ms; iter 43900: train loss 0.50446\n",
            "iter_dt 26.17ms; iter 44000: train loss 0.57772\n",
            "iter_dt 18.56ms; iter 44100: train loss 0.66428\n",
            "iter_dt 13.21ms; iter 44200: train loss 0.62089\n",
            "iter_dt 16.99ms; iter 44300: train loss 0.44145\n",
            "iter_dt 23.84ms; iter 44400: train loss 0.75302\n",
            "iter_dt 17.31ms; iter 44500: train loss 0.95776\n",
            "iter_dt 22.30ms; iter 44600: train loss 0.73752\n",
            "iter_dt 26.75ms; iter 44700: train loss 0.77149\n",
            "iter_dt 18.61ms; iter 44800: train loss 0.63996\n",
            "iter_dt 32.97ms; iter 44900: train loss 0.23164\n",
            "iter_dt 18.93ms; iter 45000: train loss 0.39323\n",
            "iter_dt 24.38ms; iter 45100: train loss 0.87170\n",
            "iter_dt 22.04ms; iter 45200: train loss 0.78937\n",
            "iter_dt 13.58ms; iter 45300: train loss 0.73238\n",
            "iter_dt 28.30ms; iter 45400: train loss 1.02269\n",
            "iter_dt 21.83ms; iter 45500: train loss 0.60221\n",
            "iter_dt 18.59ms; iter 45600: train loss 0.66195\n",
            "iter_dt 19.92ms; iter 45700: train loss 1.31184\n",
            "iter_dt 33.81ms; iter 45800: train loss 0.65322\n",
            "iter_dt 30.65ms; iter 45900: train loss 0.69281\n",
            "iter_dt 41.47ms; iter 46000: train loss 0.97822\n",
            "iter_dt 38.18ms; iter 46100: train loss 0.64186\n",
            "iter_dt 19.37ms; iter 46200: train loss 0.90313\n",
            "iter_dt 19.00ms; iter 46300: train loss 0.58214\n",
            "iter_dt 29.16ms; iter 46400: train loss 0.59997\n",
            "iter_dt 24.01ms; iter 46500: train loss 0.23496\n",
            "iter_dt 28.31ms; iter 46600: train loss 0.20693\n",
            "iter_dt 18.68ms; iter 46700: train loss 0.73730\n",
            "iter_dt 22.38ms; iter 46800: train loss 0.40426\n",
            "iter_dt 26.29ms; iter 46900: train loss 1.10503\n",
            "iter_dt 20.68ms; iter 47000: train loss 0.78755\n",
            "iter_dt 17.44ms; iter 47100: train loss 0.21638\n",
            "iter_dt 16.41ms; iter 47200: train loss 0.68517\n",
            "iter_dt 22.32ms; iter 47300: train loss 0.77667\n",
            "iter_dt 28.35ms; iter 47400: train loss 0.61641\n",
            "iter_dt 26.60ms; iter 47500: train loss 0.50774\n",
            "iter_dt 29.77ms; iter 47600: train loss 0.48752\n",
            "iter_dt 20.02ms; iter 47700: train loss 1.04308\n",
            "iter_dt 26.16ms; iter 47800: train loss 0.36190\n",
            "iter_dt 26.66ms; iter 47900: train loss 0.37426\n",
            "iter_dt 34.56ms; iter 48000: train loss 0.73337\n",
            "iter_dt 76.93ms; iter 48100: train loss 1.10924\n",
            "iter_dt 21.17ms; iter 48200: train loss 0.57723\n",
            "iter_dt 25.64ms; iter 48300: train loss 0.47959\n",
            "iter_dt 78.68ms; iter 48400: train loss 0.96683\n",
            "iter_dt 14.85ms; iter 48500: train loss 0.81661\n",
            "iter_dt 23.01ms; iter 48600: train loss 1.11910\n",
            "iter_dt 38.53ms; iter 48700: train loss 0.99971\n",
            "iter_dt 17.77ms; iter 48800: train loss 1.12193\n",
            "iter_dt 21.82ms; iter 48900: train loss 0.41027\n",
            "iter_dt 76.38ms; iter 49000: train loss 0.70625\n",
            "iter_dt 17.44ms; iter 49100: train loss 0.87959\n",
            "iter_dt 25.42ms; iter 49200: train loss 0.70446\n",
            "iter_dt 24.49ms; iter 49300: train loss 0.45080\n",
            "iter_dt 21.06ms; iter 49400: train loss 0.63712\n",
            "iter_dt 25.85ms; iter 49500: train loss 0.67307\n",
            "iter_dt 14.00ms; iter 49600: train loss 0.95845\n",
            "iter_dt 20.95ms; iter 49700: train loss 0.81257\n",
            "iter_dt 21.05ms; iter 49800: train loss 0.17365\n",
            "iter_dt 20.89ms; iter 49900: train loss 0.52991\n",
            "iter_dt 24.16ms; iter 50000: train loss 0.67514\n",
            "iter_dt 15.89ms; iter 50100: train loss 0.89705\n",
            "iter_dt 18.95ms; iter 50200: train loss 1.07721\n",
            "iter_dt 26.38ms; iter 50300: train loss 0.87148\n",
            "iter_dt 15.28ms; iter 50400: train loss 0.70994\n",
            "iter_dt 23.02ms; iter 50500: train loss 0.57091\n",
            "iter_dt 15.50ms; iter 50600: train loss 0.75356\n",
            "iter_dt 27.25ms; iter 50700: train loss 0.73346\n",
            "iter_dt 23.01ms; iter 50800: train loss 0.64257\n",
            "iter_dt 34.42ms; iter 50900: train loss 0.68146\n",
            "iter_dt 34.66ms; iter 51000: train loss 1.13543\n",
            "iter_dt 20.56ms; iter 51100: train loss 0.29817\n",
            "iter_dt 18.30ms; iter 51200: train loss 0.69583\n",
            "iter_dt 21.68ms; iter 51300: train loss 0.63875\n",
            "iter_dt 23.76ms; iter 51400: train loss 0.60076\n",
            "iter_dt 25.93ms; iter 51500: train loss 0.72994\n",
            "iter_dt 15.89ms; iter 51600: train loss 0.62915\n",
            "iter_dt 14.88ms; iter 51700: train loss 0.49088\n",
            "iter_dt 20.84ms; iter 51800: train loss 0.55443\n",
            "iter_dt 26.34ms; iter 51900: train loss 0.90346\n",
            "iter_dt 20.96ms; iter 52000: train loss 0.53734\n",
            "iter_dt 20.77ms; iter 52100: train loss 0.74767\n",
            "iter_dt 15.37ms; iter 52200: train loss 0.99475\n",
            "iter_dt 54.39ms; iter 52300: train loss 0.30117\n",
            "iter_dt 20.79ms; iter 52400: train loss 1.14890\n",
            "iter_dt 21.47ms; iter 52500: train loss 0.66059\n",
            "iter_dt 15.64ms; iter 52600: train loss 0.21644\n",
            "iter_dt 22.98ms; iter 52700: train loss 0.55948\n",
            "iter_dt 27.40ms; iter 52800: train loss 0.98566\n",
            "iter_dt 30.38ms; iter 52900: train loss 0.91954\n",
            "iter_dt 34.05ms; iter 53000: train loss 0.54834\n",
            "iter_dt 19.56ms; iter 53100: train loss 0.75367\n",
            "iter_dt 28.94ms; iter 53200: train loss 0.58705\n",
            "iter_dt 27.64ms; iter 53300: train loss 0.59170\n",
            "iter_dt 22.50ms; iter 53400: train loss 0.42074\n",
            "iter_dt 41.71ms; iter 53500: train loss 0.77766\n",
            "iter_dt 65.87ms; iter 53600: train loss 0.76694\n",
            "iter_dt 29.75ms; iter 53700: train loss 0.72968\n",
            "iter_dt 22.74ms; iter 53800: train loss 0.54844\n",
            "iter_dt 21.22ms; iter 53900: train loss 1.18162\n",
            "iter_dt 17.92ms; iter 54000: train loss 0.60273\n",
            "iter_dt 24.83ms; iter 54100: train loss 0.87670\n",
            "iter_dt 23.09ms; iter 54200: train loss 0.48541\n",
            "iter_dt 22.88ms; iter 54300: train loss 0.79352\n",
            "iter_dt 21.10ms; iter 54400: train loss 0.46640\n",
            "iter_dt 26.38ms; iter 54500: train loss 0.79138\n",
            "iter_dt 19.67ms; iter 54600: train loss 0.78860\n",
            "iter_dt 23.65ms; iter 54700: train loss 0.50807\n",
            "iter_dt 18.94ms; iter 54800: train loss 0.64187\n",
            "iter_dt 26.11ms; iter 54900: train loss 0.73760\n",
            "iter_dt 22.01ms; iter 55000: train loss 0.54360\n",
            "iter_dt 24.67ms; iter 55100: train loss 0.67405\n",
            "iter_dt 19.50ms; iter 55200: train loss 0.68464\n",
            "iter_dt 24.09ms; iter 55300: train loss 0.57895\n",
            "iter_dt 24.22ms; iter 55400: train loss 0.62199\n",
            "iter_dt 14.74ms; iter 55500: train loss 0.41304\n",
            "iter_dt 58.76ms; iter 55600: train loss 0.79918\n",
            "iter_dt 31.17ms; iter 55700: train loss 0.83698\n",
            "iter_dt 25.71ms; iter 55800: train loss 0.85440\n",
            "iter_dt 19.07ms; iter 55900: train loss 0.61101\n",
            "iter_dt 20.39ms; iter 56000: train loss 1.13658\n",
            "iter_dt 22.39ms; iter 56100: train loss 0.68912\n",
            "iter_dt 20.73ms; iter 56200: train loss 0.68409\n",
            "iter_dt 25.33ms; iter 56300: train loss 0.77469\n",
            "iter_dt 18.54ms; iter 56400: train loss 0.56129\n",
            "iter_dt 31.08ms; iter 56500: train loss 0.35616\n",
            "iter_dt 24.44ms; iter 56600: train loss 0.39965\n",
            "iter_dt 27.78ms; iter 56700: train loss 0.53371\n",
            "iter_dt 26.52ms; iter 56800: train loss 0.75092\n",
            "iter_dt 27.57ms; iter 56900: train loss 0.65769\n",
            "iter_dt 17.79ms; iter 57000: train loss 0.68821\n",
            "iter_dt 79.31ms; iter 57100: train loss 0.81596\n",
            "iter_dt 23.25ms; iter 57200: train loss 0.73120\n",
            "iter_dt 32.59ms; iter 57300: train loss 0.40198\n",
            "iter_dt 25.45ms; iter 57400: train loss 0.49698\n",
            "iter_dt 29.19ms; iter 57500: train loss 0.56251\n",
            "iter_dt 34.06ms; iter 57600: train loss 1.16964\n",
            "iter_dt 33.16ms; iter 57700: train loss 1.10094\n",
            "iter_dt 29.63ms; iter 57800: train loss 0.74016\n",
            "iter_dt 21.62ms; iter 57900: train loss 0.15611\n",
            "iter_dt 26.63ms; iter 58000: train loss 0.57566\n",
            "iter_dt 33.14ms; iter 58100: train loss 0.66367\n",
            "iter_dt 27.66ms; iter 58200: train loss 0.43670\n",
            "iter_dt 28.93ms; iter 58300: train loss 0.36727\n",
            "iter_dt 20.49ms; iter 58400: train loss 0.88022\n",
            "iter_dt 25.97ms; iter 58500: train loss 0.96144\n",
            "iter_dt 15.37ms; iter 58600: train loss 1.08719\n",
            "iter_dt 27.11ms; iter 58700: train loss 0.97628\n",
            "iter_dt 41.77ms; iter 58800: train loss 0.77192\n",
            "iter_dt 23.22ms; iter 58900: train loss 0.89645\n",
            "iter_dt 22.81ms; iter 59000: train loss 0.66017\n",
            "iter_dt 33.68ms; iter 59100: train loss 0.41205\n",
            "iter_dt 27.15ms; iter 59200: train loss 0.67823\n",
            "iter_dt 25.85ms; iter 59300: train loss 0.96548\n",
            "iter_dt 34.30ms; iter 59400: train loss 0.48039\n",
            "iter_dt 20.13ms; iter 59500: train loss 0.96188\n",
            "iter_dt 22.88ms; iter 59600: train loss 0.79414\n",
            "iter_dt 30.10ms; iter 59700: train loss 0.59922\n",
            "iter_dt 28.24ms; iter 59800: train loss 0.74955\n",
            "iter_dt 35.25ms; iter 59900: train loss 0.81194\n",
            "iter_dt 27.00ms; iter 60000: train loss 1.09725\n",
            "iter_dt 21.60ms; iter 60100: train loss 0.65814\n",
            "iter_dt 25.94ms; iter 60200: train loss 0.38913\n",
            "iter_dt 41.94ms; iter 60300: train loss 1.06503\n",
            "iter_dt 22.74ms; iter 60400: train loss 0.82295\n",
            "iter_dt 22.18ms; iter 60500: train loss 0.56979\n",
            "iter_dt 20.38ms; iter 60600: train loss 0.61294\n",
            "iter_dt 22.83ms; iter 60700: train loss 0.62311\n",
            "iter_dt 14.97ms; iter 60800: train loss 0.75673\n",
            "iter_dt 30.37ms; iter 60900: train loss 0.92063\n",
            "iter_dt 31.42ms; iter 61000: train loss 1.09012\n",
            "iter_dt 22.17ms; iter 61100: train loss 0.31873\n",
            "iter_dt 35.53ms; iter 61200: train loss 0.65819\n",
            "iter_dt 23.95ms; iter 61300: train loss 0.67294\n",
            "iter_dt 18.67ms; iter 61400: train loss 1.01229\n",
            "iter_dt 58.91ms; iter 61500: train loss 0.75080\n",
            "iter_dt 22.23ms; iter 61600: train loss 0.67307\n",
            "iter_dt 18.40ms; iter 61700: train loss 0.47607\n",
            "iter_dt 23.14ms; iter 61800: train loss 0.28703\n",
            "iter_dt 24.02ms; iter 61900: train loss 0.66342\n",
            "iter_dt 13.66ms; iter 62000: train loss 0.22293\n",
            "iter_dt 25.92ms; iter 62100: train loss 0.65483\n",
            "iter_dt 25.19ms; iter 62200: train loss 0.92408\n",
            "iter_dt 26.19ms; iter 62300: train loss 0.56208\n",
            "iter_dt 24.15ms; iter 62400: train loss 0.51990\n",
            "iter_dt 19.29ms; iter 62500: train loss 0.49175\n",
            "iter_dt 33.53ms; iter 62600: train loss 0.63347\n",
            "iter_dt 19.80ms; iter 62700: train loss 0.62970\n",
            "iter_dt 17.52ms; iter 62800: train loss 0.46212\n",
            "iter_dt 14.20ms; iter 62900: train loss 0.83436\n",
            "iter_dt 51.84ms; iter 63000: train loss 0.73841\n",
            "iter_dt 17.12ms; iter 63100: train loss 0.56749\n",
            "iter_dt 17.11ms; iter 63200: train loss 0.74421\n",
            "iter_dt 18.70ms; iter 63300: train loss 0.55248\n",
            "iter_dt 12.38ms; iter 63400: train loss 0.52115\n",
            "iter_dt 22.21ms; iter 63500: train loss 0.69244\n",
            "iter_dt 24.57ms; iter 63600: train loss 0.60406\n",
            "iter_dt 18.85ms; iter 63700: train loss 0.65811\n",
            "iter_dt 24.43ms; iter 63800: train loss 0.69401\n",
            "iter_dt 17.64ms; iter 63900: train loss 0.33639\n",
            "iter_dt 37.66ms; iter 64000: train loss 1.03076\n",
            "iter_dt 18.25ms; iter 64100: train loss 0.71061\n",
            "iter_dt 15.82ms; iter 64200: train loss 0.65938\n",
            "iter_dt 16.28ms; iter 64300: train loss 0.54016\n",
            "iter_dt 15.68ms; iter 64400: train loss 1.17434\n",
            "iter_dt 67.20ms; iter 64500: train loss 0.76244\n",
            "iter_dt 40.23ms; iter 64600: train loss 1.11610\n",
            "iter_dt 18.74ms; iter 64700: train loss 1.16742\n",
            "iter_dt 13.63ms; iter 64800: train loss 0.58869\n",
            "iter_dt 22.77ms; iter 64900: train loss 0.83463\n",
            "iter_dt 25.57ms; iter 65000: train loss 0.81764\n",
            "iter_dt 22.23ms; iter 65100: train loss 0.50930\n",
            "iter_dt 25.38ms; iter 65200: train loss 0.58543\n",
            "iter_dt 53.65ms; iter 65300: train loss 0.87188\n",
            "iter_dt 34.62ms; iter 65400: train loss 0.60852\n",
            "iter_dt 34.14ms; iter 65500: train loss 0.79965\n",
            "iter_dt 19.84ms; iter 65600: train loss 0.47947\n",
            "iter_dt 43.44ms; iter 65700: train loss 0.66966\n",
            "iter_dt 26.88ms; iter 65800: train loss 0.24124\n",
            "iter_dt 66.03ms; iter 65900: train loss 0.69225\n",
            "iter_dt 35.13ms; iter 66000: train loss 0.55041\n",
            "iter_dt 28.39ms; iter 66100: train loss 1.08225\n",
            "iter_dt 27.15ms; iter 66200: train loss 0.87511\n",
            "iter_dt 24.87ms; iter 66300: train loss 0.65163\n",
            "iter_dt 19.45ms; iter 66400: train loss 0.77996\n",
            "iter_dt 26.66ms; iter 66500: train loss 0.60520\n",
            "iter_dt 15.29ms; iter 66600: train loss 0.66081\n",
            "iter_dt 30.92ms; iter 66700: train loss 0.53638\n",
            "iter_dt 26.34ms; iter 66800: train loss 0.23765\n",
            "iter_dt 24.90ms; iter 66900: train loss 0.59101\n",
            "iter_dt 30.13ms; iter 67000: train loss 0.50182\n",
            "iter_dt 14.78ms; iter 67100: train loss 0.83625\n",
            "iter_dt 18.71ms; iter 67200: train loss 0.41363\n",
            "iter_dt 24.69ms; iter 67300: train loss 0.24357\n",
            "iter_dt 18.25ms; iter 67400: train loss 0.64836\n",
            "iter_dt 29.11ms; iter 67500: train loss 0.65327\n",
            "iter_dt 20.82ms; iter 67600: train loss 0.56113\n",
            "iter_dt 33.56ms; iter 67700: train loss 0.46055\n",
            "iter_dt 40.97ms; iter 67800: train loss 0.99956\n",
            "iter_dt 14.59ms; iter 67900: train loss 0.54386\n",
            "iter_dt 20.94ms; iter 68000: train loss 0.74051\n",
            "iter_dt 35.46ms; iter 68100: train loss 0.85279\n",
            "iter_dt 34.35ms; iter 68200: train loss 0.26171\n",
            "iter_dt 52.81ms; iter 68300: train loss 0.37649\n",
            "iter_dt 17.45ms; iter 68400: train loss 0.88240\n",
            "iter_dt 16.84ms; iter 68500: train loss 0.94494\n",
            "iter_dt 34.14ms; iter 68600: train loss 0.79939\n",
            "iter_dt 16.86ms; iter 68700: train loss 0.81656\n",
            "iter_dt 28.10ms; iter 68800: train loss 0.53364\n",
            "iter_dt 23.59ms; iter 68900: train loss 0.90753\n",
            "iter_dt 18.33ms; iter 69000: train loss 0.60309\n",
            "iter_dt 19.51ms; iter 69100: train loss 0.64478\n",
            "iter_dt 19.28ms; iter 69200: train loss 0.19257\n",
            "iter_dt 23.69ms; iter 69300: train loss 0.40421\n",
            "iter_dt 23.78ms; iter 69400: train loss 0.82832\n",
            "iter_dt 19.04ms; iter 69500: train loss 0.56346\n",
            "iter_dt 19.77ms; iter 69600: train loss 1.16874\n",
            "iter_dt 24.19ms; iter 69700: train loss 0.51897\n",
            "iter_dt 22.68ms; iter 69800: train loss 1.02919\n",
            "iter_dt 32.91ms; iter 69900: train loss 0.67523\n",
            "iter_dt 36.41ms; iter 70000: train loss 0.54655\n",
            "iter_dt 40.93ms; iter 70100: train loss 0.78447\n",
            "iter_dt 22.35ms; iter 70200: train loss 0.42398\n",
            "iter_dt 21.40ms; iter 70300: train loss 0.48516\n",
            "iter_dt 15.44ms; iter 70400: train loss 0.41997\n",
            "iter_dt 27.79ms; iter 70500: train loss 0.92358\n",
            "iter_dt 34.55ms; iter 70600: train loss 0.83794\n",
            "iter_dt 15.14ms; iter 70700: train loss 0.87158\n",
            "iter_dt 26.58ms; iter 70800: train loss 0.15222\n",
            "iter_dt 76.78ms; iter 70900: train loss 0.64643\n",
            "iter_dt 13.03ms; iter 71000: train loss 0.52829\n",
            "iter_dt 20.65ms; iter 71100: train loss 0.98840\n",
            "iter_dt 31.48ms; iter 71200: train loss 0.44494\n",
            "iter_dt 23.96ms; iter 71300: train loss 0.66291\n",
            "iter_dt 16.36ms; iter 71400: train loss 0.76765\n",
            "iter_dt 39.68ms; iter 71500: train loss 0.79435\n",
            "iter_dt 65.98ms; iter 71600: train loss 1.06260\n",
            "iter_dt 31.99ms; iter 71700: train loss 0.40696\n",
            "iter_dt 25.46ms; iter 71800: train loss 0.69206\n",
            "iter_dt 21.47ms; iter 71900: train loss 0.51023\n",
            "iter_dt 26.52ms; iter 72000: train loss 0.12125\n",
            "iter_dt 44.48ms; iter 72100: train loss 0.56336\n",
            "iter_dt 30.49ms; iter 72200: train loss 0.94615\n",
            "iter_dt 31.30ms; iter 72300: train loss 0.60365\n",
            "iter_dt 32.70ms; iter 72400: train loss 0.52555\n",
            "iter_dt 33.44ms; iter 72500: train loss 0.65730\n",
            "iter_dt 15.38ms; iter 72600: train loss 1.00967\n",
            "iter_dt 19.24ms; iter 72700: train loss 0.96891\n",
            "iter_dt 22.37ms; iter 72800: train loss 0.63679\n",
            "iter_dt 15.82ms; iter 72900: train loss 0.63402\n",
            "iter_dt 146.08ms; iter 73000: train loss 0.61045\n",
            "iter_dt 18.53ms; iter 73100: train loss 0.43271\n",
            "iter_dt 32.92ms; iter 73200: train loss 0.52526\n",
            "iter_dt 19.21ms; iter 73300: train loss 0.73017\n",
            "iter_dt 22.38ms; iter 73400: train loss 0.62816\n",
            "iter_dt 18.68ms; iter 73500: train loss 0.42750\n",
            "iter_dt 28.78ms; iter 73600: train loss 0.53769\n",
            "iter_dt 39.43ms; iter 73700: train loss 1.01472\n",
            "iter_dt 21.82ms; iter 73800: train loss 0.63594\n",
            "iter_dt 29.55ms; iter 73900: train loss 0.65398\n",
            "iter_dt 26.84ms; iter 74000: train loss 0.67832\n",
            "iter_dt 31.58ms; iter 74100: train loss 0.77040\n",
            "iter_dt 27.73ms; iter 74200: train loss 0.52453\n",
            "iter_dt 14.66ms; iter 74300: train loss 0.33034\n",
            "iter_dt 25.70ms; iter 74400: train loss 0.79824\n",
            "iter_dt 21.45ms; iter 74500: train loss 0.27311\n",
            "iter_dt 21.91ms; iter 74600: train loss 0.24349\n",
            "iter_dt 26.22ms; iter 74700: train loss 0.57876\n",
            "iter_dt 21.79ms; iter 74800: train loss 0.79026\n",
            "iter_dt 24.34ms; iter 74900: train loss 0.70993\n",
            "iter_dt 33.11ms; iter 75000: train loss 0.82564\n",
            "iter_dt 24.59ms; iter 75100: train loss 0.78443\n",
            "iter_dt 33.30ms; iter 75200: train loss 0.34690\n",
            "iter_dt 20.25ms; iter 75300: train loss 0.64745\n",
            "iter_dt 25.94ms; iter 75400: train loss 0.66041\n",
            "iter_dt 23.14ms; iter 75500: train loss 0.58544\n",
            "iter_dt 21.82ms; iter 75600: train loss 0.32334\n",
            "iter_dt 87.22ms; iter 75700: train loss 0.71628\n",
            "iter_dt 145.12ms; iter 75800: train loss 0.26452\n",
            "iter_dt 34.84ms; iter 75900: train loss 0.74438\n",
            "iter_dt 25.57ms; iter 76000: train loss 0.78027\n",
            "iter_dt 26.77ms; iter 76100: train loss 0.99292\n",
            "iter_dt 17.45ms; iter 76200: train loss 1.18501\n",
            "iter_dt 18.92ms; iter 76300: train loss 0.46337\n",
            "iter_dt 25.08ms; iter 76400: train loss 1.17175\n",
            "iter_dt 29.89ms; iter 76500: train loss 0.91841\n",
            "iter_dt 16.85ms; iter 76600: train loss 0.71332\n",
            "iter_dt 18.76ms; iter 76700: train loss 0.85685\n",
            "iter_dt 16.70ms; iter 76800: train loss 0.48702\n",
            "iter_dt 27.09ms; iter 76900: train loss 0.45892\n",
            "iter_dt 27.95ms; iter 77000: train loss 0.60692\n",
            "iter_dt 18.46ms; iter 77100: train loss 0.48779\n",
            "iter_dt 28.79ms; iter 77200: train loss 0.52795\n",
            "iter_dt 20.33ms; iter 77300: train loss 0.24133\n",
            "iter_dt 18.10ms; iter 77400: train loss 0.62247\n",
            "iter_dt 27.79ms; iter 77500: train loss 0.74057\n",
            "iter_dt 21.67ms; iter 77600: train loss 0.90376\n",
            "iter_dt 28.80ms; iter 77700: train loss 0.94194\n",
            "iter_dt 52.92ms; iter 77800: train loss 0.61428\n",
            "iter_dt 25.81ms; iter 77900: train loss 0.18767\n",
            "iter_dt 33.28ms; iter 78000: train loss 0.51760\n",
            "iter_dt 21.87ms; iter 78100: train loss 1.06940\n",
            "iter_dt 40.62ms; iter 78200: train loss 0.74245\n",
            "iter_dt 17.81ms; iter 78300: train loss 0.58806\n",
            "iter_dt 24.34ms; iter 78400: train loss 0.68091\n",
            "iter_dt 21.25ms; iter 78500: train loss 0.65419\n",
            "iter_dt 22.20ms; iter 78600: train loss 0.54802\n",
            "iter_dt 34.18ms; iter 78700: train loss 0.80926\n",
            "iter_dt 23.95ms; iter 78800: train loss 0.82350\n",
            "iter_dt 16.26ms; iter 78900: train loss 0.90939\n",
            "iter_dt 19.68ms; iter 79000: train loss 0.32402\n",
            "iter_dt 40.00ms; iter 79100: train loss 0.55518\n",
            "iter_dt 24.08ms; iter 79200: train loss 0.39658\n",
            "iter_dt 19.83ms; iter 79300: train loss 0.40281\n",
            "iter_dt 17.06ms; iter 79400: train loss 0.86615\n",
            "iter_dt 22.22ms; iter 79500: train loss 0.40091\n",
            "iter_dt 146.73ms; iter 79600: train loss 0.52495\n",
            "iter_dt 40.82ms; iter 79700: train loss 0.46888\n",
            "iter_dt 18.26ms; iter 79800: train loss 0.47116\n",
            "iter_dt 26.74ms; iter 79900: train loss 0.72640\n",
            "iter_dt 22.19ms; iter 80000: train loss 0.40420\n",
            "iter_dt 29.03ms; iter 80100: train loss 0.58183\n",
            "iter_dt 20.72ms; iter 80200: train loss 0.51470\n",
            "iter_dt 21.15ms; iter 80300: train loss 0.53435\n",
            "iter_dt 40.24ms; iter 80400: train loss 0.60555\n",
            "iter_dt 146.41ms; iter 80500: train loss 0.89003\n",
            "iter_dt 17.59ms; iter 80600: train loss 0.72634\n",
            "iter_dt 39.91ms; iter 80700: train loss 0.79408\n",
            "iter_dt 23.97ms; iter 80800: train loss 0.46623\n",
            "iter_dt 20.51ms; iter 80900: train loss 0.18749\n",
            "iter_dt 32.65ms; iter 81000: train loss 0.70343\n",
            "iter_dt 54.59ms; iter 81100: train loss 0.68067\n",
            "iter_dt 101.41ms; iter 81200: train loss 0.88252\n",
            "iter_dt 31.59ms; iter 81300: train loss 1.00349\n",
            "iter_dt 19.50ms; iter 81400: train loss 0.77644\n",
            "iter_dt 21.94ms; iter 81500: train loss 0.66286\n",
            "iter_dt 19.16ms; iter 81600: train loss 0.56770\n",
            "iter_dt 22.41ms; iter 81700: train loss 0.80223\n",
            "iter_dt 37.09ms; iter 81800: train loss 0.72569\n",
            "iter_dt 18.64ms; iter 81900: train loss 0.76363\n",
            "iter_dt 21.01ms; iter 82000: train loss 0.71389\n",
            "iter_dt 147.31ms; iter 82100: train loss 0.60859\n",
            "iter_dt 12.91ms; iter 82200: train loss 0.51879\n",
            "iter_dt 17.27ms; iter 82300: train loss 0.59552\n",
            "iter_dt 22.36ms; iter 82400: train loss 0.25317\n",
            "iter_dt 39.13ms; iter 82500: train loss 0.43824\n",
            "iter_dt 25.17ms; iter 82600: train loss 0.66564\n",
            "iter_dt 43.79ms; iter 82700: train loss 0.58415\n",
            "iter_dt 19.63ms; iter 82800: train loss 0.49797\n",
            "iter_dt 21.92ms; iter 82900: train loss 0.58315\n",
            "iter_dt 25.52ms; iter 83000: train loss 0.88339\n",
            "iter_dt 22.44ms; iter 83100: train loss 0.50836\n",
            "iter_dt 29.50ms; iter 83200: train loss 0.34509\n",
            "iter_dt 15.75ms; iter 83300: train loss 0.57155\n",
            "iter_dt 18.97ms; iter 83400: train loss 0.28322\n",
            "iter_dt 32.82ms; iter 83500: train loss 0.37503\n",
            "iter_dt 52.31ms; iter 83600: train loss 0.82579\n",
            "iter_dt 22.60ms; iter 83700: train loss 0.48267\n",
            "iter_dt 28.15ms; iter 83800: train loss 0.79666\n",
            "iter_dt 23.30ms; iter 83900: train loss 0.64092\n",
            "iter_dt 39.36ms; iter 84000: train loss 0.43333\n",
            "iter_dt 20.62ms; iter 84100: train loss 0.56920\n",
            "iter_dt 15.40ms; iter 84200: train loss 0.71793\n",
            "iter_dt 17.43ms; iter 84300: train loss 0.69654\n",
            "iter_dt 18.35ms; iter 84400: train loss 0.44998\n",
            "iter_dt 16.71ms; iter 84500: train loss 0.52704\n",
            "iter_dt 22.79ms; iter 84600: train loss 0.54599\n",
            "iter_dt 23.01ms; iter 84700: train loss 0.20545\n",
            "iter_dt 55.65ms; iter 84800: train loss 0.47212\n",
            "iter_dt 28.04ms; iter 84900: train loss 0.46398\n",
            "iter_dt 22.13ms; iter 85000: train loss 0.57400\n",
            "iter_dt 25.65ms; iter 85100: train loss 1.22720\n",
            "iter_dt 19.48ms; iter 85200: train loss 0.41021\n",
            "iter_dt 18.30ms; iter 85300: train loss 0.36674\n",
            "iter_dt 20.89ms; iter 85400: train loss 0.48564\n",
            "iter_dt 21.16ms; iter 85500: train loss 0.67224\n",
            "iter_dt 17.12ms; iter 85600: train loss 0.50033\n",
            "iter_dt 17.25ms; iter 85700: train loss 0.50058\n",
            "iter_dt 144.01ms; iter 85800: train loss 0.53383\n",
            "iter_dt 18.92ms; iter 85900: train loss 0.91765\n",
            "iter_dt 144.16ms; iter 86000: train loss 0.57924\n",
            "iter_dt 45.48ms; iter 86100: train loss 0.80310\n",
            "iter_dt 17.14ms; iter 86200: train loss 0.60104\n",
            "iter_dt 17.25ms; iter 86300: train loss 0.26788\n",
            "iter_dt 25.97ms; iter 86400: train loss 0.97465\n",
            "iter_dt 35.78ms; iter 86500: train loss 0.62108\n",
            "iter_dt 26.14ms; iter 86600: train loss 0.73219\n",
            "iter_dt 24.52ms; iter 86700: train loss 0.63698\n",
            "iter_dt 27.07ms; iter 86800: train loss 0.80782\n",
            "iter_dt 31.91ms; iter 86900: train loss 0.68505\n",
            "iter_dt 15.47ms; iter 87000: train loss 0.46049\n",
            "iter_dt 23.87ms; iter 87100: train loss 0.58636\n",
            "iter_dt 144.54ms; iter 87200: train loss 0.64322\n",
            "iter_dt 16.73ms; iter 87300: train loss 1.03435\n",
            "iter_dt 22.45ms; iter 87400: train loss 0.50379\n",
            "iter_dt 101.96ms; iter 87500: train loss 0.77950\n",
            "iter_dt 20.95ms; iter 87600: train loss 0.67503\n",
            "iter_dt 17.29ms; iter 87700: train loss 0.59465\n",
            "iter_dt 27.85ms; iter 87800: train loss 0.71137\n",
            "iter_dt 26.47ms; iter 87900: train loss 0.65935\n",
            "iter_dt 25.15ms; iter 88000: train loss 1.00986\n",
            "iter_dt 21.67ms; iter 88100: train loss 0.82220\n",
            "iter_dt 22.52ms; iter 88200: train loss 0.93963\n",
            "iter_dt 22.33ms; iter 88300: train loss 0.57150\n",
            "iter_dt 26.24ms; iter 88400: train loss 0.66872\n",
            "iter_dt 19.01ms; iter 88500: train loss 0.51588\n",
            "iter_dt 19.08ms; iter 88600: train loss 0.47542\n",
            "iter_dt 22.26ms; iter 88700: train loss 0.86221\n",
            "iter_dt 19.07ms; iter 88800: train loss 0.64563\n",
            "iter_dt 16.25ms; iter 88900: train loss 0.48557\n",
            "iter_dt 32.39ms; iter 89000: train loss 0.79989\n",
            "iter_dt 19.64ms; iter 89100: train loss 0.58297\n",
            "iter_dt 33.78ms; iter 89200: train loss 0.82131\n",
            "iter_dt 16.64ms; iter 89300: train loss 0.85455\n",
            "iter_dt 15.65ms; iter 89400: train loss 0.65961\n",
            "iter_dt 28.18ms; iter 89500: train loss 0.88420\n",
            "iter_dt 18.97ms; iter 89600: train loss 0.60587\n",
            "iter_dt 21.24ms; iter 89700: train loss 0.32563\n",
            "iter_dt 22.81ms; iter 89800: train loss 0.80975\n",
            "iter_dt 39.85ms; iter 89900: train loss 0.81477\n",
            "iter_dt 21.48ms; iter 90000: train loss 0.79633\n",
            "iter_dt 20.06ms; iter 90100: train loss 0.85178\n",
            "iter_dt 28.22ms; iter 90200: train loss 0.70568\n",
            "iter_dt 29.19ms; iter 90300: train loss 0.68136\n",
            "iter_dt 23.86ms; iter 90400: train loss 0.99900\n",
            "iter_dt 19.20ms; iter 90500: train loss 0.33231\n",
            "iter_dt 23.26ms; iter 90600: train loss 0.87281\n",
            "iter_dt 22.27ms; iter 90700: train loss 0.66830\n",
            "iter_dt 33.45ms; iter 90800: train loss 0.70390\n",
            "iter_dt 29.02ms; iter 90900: train loss 0.59366\n",
            "iter_dt 32.28ms; iter 91000: train loss 0.83530\n",
            "iter_dt 21.04ms; iter 91100: train loss 0.47584\n",
            "iter_dt 29.45ms; iter 91200: train loss 0.52310\n",
            "iter_dt 22.76ms; iter 91300: train loss 0.72335\n",
            "iter_dt 22.34ms; iter 91400: train loss 1.01181\n",
            "iter_dt 22.00ms; iter 91500: train loss 0.23610\n",
            "iter_dt 21.43ms; iter 91600: train loss 0.65044\n",
            "iter_dt 24.66ms; iter 91700: train loss 0.48575\n",
            "iter_dt 17.12ms; iter 91800: train loss 0.76369\n",
            "iter_dt 28.53ms; iter 91900: train loss 0.74007\n",
            "iter_dt 17.65ms; iter 92000: train loss 0.72019\n",
            "iter_dt 21.50ms; iter 92100: train loss 0.70225\n",
            "iter_dt 67.26ms; iter 92200: train loss 0.99014\n",
            "iter_dt 34.14ms; iter 92300: train loss 0.69879\n",
            "iter_dt 28.42ms; iter 92400: train loss 0.60421\n",
            "iter_dt 16.55ms; iter 92500: train loss 0.80477\n",
            "iter_dt 16.49ms; iter 92600: train loss 0.58095\n",
            "iter_dt 23.17ms; iter 92700: train loss 0.48468\n",
            "iter_dt 52.11ms; iter 92800: train loss 0.70712\n",
            "iter_dt 65.15ms; iter 92900: train loss 0.59245\n",
            "iter_dt 17.22ms; iter 93000: train loss 0.29379\n",
            "iter_dt 20.90ms; iter 93100: train loss 0.60508\n",
            "iter_dt 26.30ms; iter 93200: train loss 1.17618\n",
            "iter_dt 76.69ms; iter 93300: train loss 0.64973\n",
            "iter_dt 20.54ms; iter 93400: train loss 1.13420\n",
            "iter_dt 12.03ms; iter 93500: train loss 0.84068\n",
            "iter_dt 31.85ms; iter 93600: train loss 0.57088\n",
            "iter_dt 27.41ms; iter 93700: train loss 0.99478\n",
            "iter_dt 21.31ms; iter 93800: train loss 0.87251\n",
            "iter_dt 19.06ms; iter 93900: train loss 0.68470\n",
            "iter_dt 21.93ms; iter 94000: train loss 0.89788\n",
            "iter_dt 64.99ms; iter 94100: train loss 0.81552\n",
            "iter_dt 20.99ms; iter 94200: train loss 1.33512\n",
            "iter_dt 16.47ms; iter 94300: train loss 0.82155\n",
            "iter_dt 19.26ms; iter 94400: train loss 0.57580\n",
            "iter_dt 25.68ms; iter 94500: train loss 0.82707\n",
            "iter_dt 19.43ms; iter 94600: train loss 0.23444\n",
            "iter_dt 26.14ms; iter 94700: train loss 0.97131\n",
            "iter_dt 20.92ms; iter 94800: train loss 0.83436\n",
            "iter_dt 27.57ms; iter 94900: train loss 0.63632\n",
            "iter_dt 13.95ms; iter 95000: train loss 0.42279\n",
            "iter_dt 39.98ms; iter 95100: train loss 0.33874\n",
            "iter_dt 22.64ms; iter 95200: train loss 0.48443\n",
            "iter_dt 15.90ms; iter 95300: train loss 0.71473\n",
            "iter_dt 20.92ms; iter 95400: train loss 0.11200\n",
            "iter_dt 25.97ms; iter 95500: train loss 0.43878\n",
            "iter_dt 35.59ms; iter 95600: train loss 0.54640\n",
            "iter_dt 22.10ms; iter 95700: train loss 0.44686\n",
            "iter_dt 26.11ms; iter 95800: train loss 0.52424\n",
            "iter_dt 27.62ms; iter 95900: train loss 0.51143\n",
            "iter_dt 22.07ms; iter 96000: train loss 0.58643\n",
            "iter_dt 19.88ms; iter 96100: train loss 0.37810\n",
            "iter_dt 17.40ms; iter 96200: train loss 0.71099\n",
            "iter_dt 18.42ms; iter 96300: train loss 0.86619\n",
            "iter_dt 26.75ms; iter 96400: train loss 0.41513\n",
            "iter_dt 33.84ms; iter 96500: train loss 1.05166\n",
            "iter_dt 24.41ms; iter 96600: train loss 0.92547\n",
            "iter_dt 31.05ms; iter 96700: train loss 0.70747\n",
            "iter_dt 31.99ms; iter 96800: train loss 0.80959\n",
            "iter_dt 22.09ms; iter 96900: train loss 0.67420\n",
            "iter_dt 21.92ms; iter 97000: train loss 0.65345\n",
            "iter_dt 20.65ms; iter 97100: train loss 0.49888\n",
            "iter_dt 22.84ms; iter 97200: train loss 0.68510\n",
            "iter_dt 19.08ms; iter 97300: train loss 0.56700\n",
            "iter_dt 22.14ms; iter 97400: train loss 1.15947\n",
            "iter_dt 29.23ms; iter 97500: train loss 0.65298\n",
            "iter_dt 15.38ms; iter 97600: train loss 0.87809\n",
            "iter_dt 20.78ms; iter 97700: train loss 0.72301\n",
            "iter_dt 22.05ms; iter 97800: train loss 0.96299\n",
            "iter_dt 34.20ms; iter 97900: train loss 0.60260\n",
            "iter_dt 29.19ms; iter 98000: train loss 0.75163\n",
            "iter_dt 42.98ms; iter 98100: train loss 0.31841\n",
            "iter_dt 20.02ms; iter 98200: train loss 0.61899\n",
            "iter_dt 18.93ms; iter 98300: train loss 1.15599\n",
            "iter_dt 17.32ms; iter 98400: train loss 0.85847\n",
            "iter_dt 28.27ms; iter 98500: train loss 0.41640\n",
            "iter_dt 20.20ms; iter 98600: train loss 0.57224\n",
            "iter_dt 24.65ms; iter 98700: train loss 0.80752\n",
            "iter_dt 20.22ms; iter 98800: train loss 0.50845\n",
            "iter_dt 145.54ms; iter 98900: train loss 0.60209\n",
            "iter_dt 20.65ms; iter 99000: train loss 0.73813\n",
            "iter_dt 19.21ms; iter 99100: train loss 1.17018\n",
            "iter_dt 24.27ms; iter 99200: train loss 0.56099\n",
            "iter_dt 32.10ms; iter 99300: train loss 0.64781\n",
            "iter_dt 22.28ms; iter 99400: train loss 0.94958\n",
            "iter_dt 25.84ms; iter 99500: train loss 0.19326\n",
            "iter_dt 30.25ms; iter 99600: train loss 0.56940\n",
            "iter_dt 22.05ms; iter 99700: train loss 0.78611\n",
            "iter_dt 24.77ms; iter 99800: train loss 0.75598\n",
            "iter_dt 22.51ms; iter 99900: train loss 0.65480\n"
          ]
        }
      ],
      "source": [
        "# This function is called at the end of every batch in training\n",
        "# and is used to report the amount of time per 100 batches, and the loss at that point\n",
        "\n",
        "def batch_end_callback(trainer):\n",
        "    if trainer.iter_num % 100 == 0:\n",
        "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "# Train!\n",
        "trainer.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxXAYehDZgiJ"
      },
      "outputs": [],
      "source": [
        "model.to(trainer.device)\n",
        "# store the saved model in a file, so can re-use later\n",
        "modelsavename= \"model_filename.pt\"  # change the name here to save in a specific file (and restore below)\n",
        "with open(modelsavename, \"wb\") as f:\n",
        "    torch.save(trainer.model.state_dict(), f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkrCYjkHZgiJ",
        "outputId": "702c4cb9-795d-4973-b53e-535869036315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'He and I hold the dog.. dog. dog and cat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Use the trained language model to predict a sequence of words following a few words\n",
        "encoded_prompt = train_dataset.tokenizer(\"He and I\").to(trainer.device)\n",
        "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.8, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHv1lufHZgiJ",
        "outputId": "41623d4c-b97b-4470-e8ef-ccb84603a8d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'She rubs and holds the cat. dog. dog. dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Another example\n",
        "encoded_prompt = train_dataset.tokenizer(\"She rubs\").to(trainer.device)\n",
        "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3**"
      ],
      "metadata": {
        "id": "EsGJ-OazMufS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_prompt = train_dataset.tokenizer(\"He and I\").to(trainer.device)\n",
        "generated_sequence,probability = trainer.model.generate0(encoded_prompt, trainer.device, temperature=0.8, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "W8X39ogSKQN7",
        "outputId": "31c2dfde-6461-4eff-8524-0e5a8c2a21cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'He and I hold the dog.. dog. dog and cat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaoMFdU6O_tW",
        "outputId": "37d76cf0-5fbe-43a7-a0aa-9f52499fad1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1544,  290,  314, 1745,  262, 3290,   13,   13, 3290,   13, 3290,  290,\n",
            "         3797]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(probability)"
      ],
      "metadata": {
        "id": "yIUGnLZGKkZ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636b17b1-e4af-4d66-9d8b-de41c2cc220b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 1.0000, 1.0000, 0.6165, 0.5366, 0.5386, 0.9104, 0.9458, 0.5335,\n",
            "         0.9421, 0.6213, 0.6957, 0.9843]], device='cuda:0',\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another example\n",
        "encoded_prompt = train_dataset.tokenizer(\"She rubs\").to(trainer.device)\n",
        "generated_sequence,probability = trainer.model.generate0(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sWEiOlCfawwK",
        "outputId": "53433bb9-0cb4-4167-d2b7-614abd6b9573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'She rubs a dog and cat. cat. dog. cat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "su-kkegDa9Eu",
        "outputId": "090aba8e-693b-4df7-f2d8-136acf8f69c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3347, 6437,   82,  257, 3290,  290, 3797,   13, 3797,   13, 3290,   13,\n",
            "         3797]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DUs4hIPa-VY",
        "outputId": "636a7f1b-f286-4ddf-f479-a1618af0a1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 1.0000, 1.0000, 0.4260, 0.5635, 0.5800, 0.9999, 0.9960, 0.7251,\n",
            "         0.9741, 0.9095, 0.9940, 0.6573]], device='cuda:0',\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# own choosing example\n",
        "encoded_prompt = train_dataset.tokenizer(\"He rubs\").to(trainer.device)\n",
        "generated_sequence,probability = trainer.model.generate0(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iJ87hG_ZbmVL",
        "outputId": "3f92b3fb-af38-4e8b-d730-b9706161809f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'He rubs a dog and cat. cat. dog. dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckc0K-bjb4em",
        "outputId": "3ceaa496-5443-4c00-b24b-a13788dc22e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1544, 6437,   82,  257, 3290,  290, 3797,   13, 3797,   13, 3290,   13,\n",
            "         3290]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(probability)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQJy9yjeb56n",
        "outputId": "f33b6ac0-1962-4bff-95e8-293bf124a379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 1.0000, 1.0000, 0.3608, 0.6248, 0.6104, 0.9994, 0.9983, 0.7570,\n",
            "         0.9853, 0.8682, 0.9914, 0.6459]], device='cuda:0',\n",
            "       grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4**"
      ],
      "metadata": {
        "id": "AMyRMF4XgZc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_prompt = train_dataset.tokenizer(\"He and I\").to(trainer.device)\n",
        "generated_sequence,worldList,probability = trainer.model.generate1(encoded_prompt, trainer.device, temperature=0.8, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5u6msYwIgbeX",
        "outputId": "11a1b9bb-3941-4a2f-88f6-1fe724220139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'He and I hold the cat.. dog. dog and dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print((worldList[1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQqAf9dTg9Lg",
        "outputId": "6a789090-71ee-41b5-d28d-381aaaf7af67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1745., 6437.,  460., 6622., 3290., 3797.],\n",
            "        [ 262.,  257.,  290., 1745., 3290., 6437.],\n",
            "        [3797., 3290., 6437.,  262.,  257.,  290.],\n",
            "        [  13.,  764.,  290., 3797., 6437., 1745.],\n",
            "        [  13.,  764., 3290., 3797., 1745., 6437.],\n",
            "        [3290., 3797.,  290.,   13.,  262.,  257.],\n",
            "        [  13.,  764.,  290., 6622., 1745., 6437.],\n",
            "        [3290., 3797.,   13.,  290.,  257.,  262.],\n",
            "        [ 290.,   13.,  257.,  262., 6622.,  460.],\n",
            "        [3290., 3797.,  290.,   13., 6622.,  262.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.tokenizer.decode(worldList[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GyBl_Zo5hYIu",
        "outputId": "6821b8b4-e791-48b3-d0e1-52030319f66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' hold rub can holds dog cat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.round(probability,decimals=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZagzTyBThez1",
        "outputId": "5aac9032-3144-4d81-add3-2f634a356ef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0000, 0.0000, -0.0000, 0.0000, -0.0000, 0.0000],\n",
            "        [0.6670, 0.2200, 0.1130, 0.0010, 0.0000, 0.0000],\n",
            "        [0.5420, 0.4560, 0.0010, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5030, 0.4970, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.9930, 0.0060, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.9320, 0.0440, 0.0160, 0.0030, 0.0020, 0.0010],\n",
            "        [0.8590, 0.1400, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.8420, 0.1500, 0.0030, 0.0020, 0.0010, 0.0010],\n",
            "        [0.7500, 0.2360, 0.0060, 0.0060, 0.0010, 0.0010],\n",
            "        [0.6150, 0.3830, 0.0010, 0.0010, 0.0000, 0.0000],\n",
            "        [0.9840, 0.0150, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "YFncFYQ-imlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(worldList.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzx9WNMmkaxK",
        "outputId": "9d7c1860-cc8c-47b8-e297-e1f575cf78dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worldList0 = worldList[1:]\n",
        "probability0 = probability[1:]\n",
        "\n",
        "table = []\n",
        "\n",
        "for i in range(worldList0.shape[0]):\n",
        "  column = []\n",
        "  for t in range(worldList0.shape[1]):\n",
        "    column.append(train_dataset.tokenizer.decode(worldList0[i][t].reshape(1)) + f\" / {probability0[i][t]:.3f}\")\n",
        "  table.append(column)\n",
        "\n",
        "df = pd.DataFrame(data=table)"
      ],
      "metadata": {
        "id": "VhpsXCgLifyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "_YQHY7d5lfc1",
        "outputId": "f2588e91-ba15-4445-d178-110db80ffb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                0              1             2              3              4  \\\n",
              "0    hold / 0.667    the / 0.542   cat / 0.503      . / 0.993      . / 0.932   \n",
              "1     rub / 0.220      a / 0.456   dog / 0.497      . / 0.006      . / 0.044   \n",
              "2     can / 0.113    and / 0.001   rub / 0.000    and / 0.000    dog / 0.016   \n",
              "3   holds / 0.001   hold / 0.000   the / 0.000    cat / 0.000    cat / 0.003   \n",
              "4     dog / 0.000    dog / 0.000     a / 0.000    rub / 0.000   hold / 0.002   \n",
              "5     cat / 0.000    rub / 0.000   and / 0.000   hold / 0.000    rub / 0.001   \n",
              "\n",
              "              5               6             7               8               9  \n",
              "0   dog / 0.859       . / 0.842   dog / 0.750     and / 0.615     dog / 0.984  \n",
              "1   cat / 0.140       . / 0.150   cat / 0.236       . / 0.383     cat / 0.015  \n",
              "2   and / 0.000     and / 0.003     . / 0.006       a / 0.001     and / 0.000  \n",
              "3     . / 0.000   holds / 0.002   and / 0.006     the / 0.001       . / 0.000  \n",
              "4   the / 0.000    hold / 0.001     a / 0.001   holds / 0.000   holds / 0.000  \n",
              "5     a / 0.000     rub / 0.001   the / 0.001     can / 0.000     the / 0.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-32cb46f9-8e08-4630-a878-811a748a534f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hold / 0.667</td>\n",
              "      <td>the / 0.542</td>\n",
              "      <td>cat / 0.503</td>\n",
              "      <td>. / 0.993</td>\n",
              "      <td>. / 0.932</td>\n",
              "      <td>dog / 0.859</td>\n",
              "      <td>. / 0.842</td>\n",
              "      <td>dog / 0.750</td>\n",
              "      <td>and / 0.615</td>\n",
              "      <td>dog / 0.984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rub / 0.220</td>\n",
              "      <td>a / 0.456</td>\n",
              "      <td>dog / 0.497</td>\n",
              "      <td>. / 0.006</td>\n",
              "      <td>. / 0.044</td>\n",
              "      <td>cat / 0.140</td>\n",
              "      <td>. / 0.150</td>\n",
              "      <td>cat / 0.236</td>\n",
              "      <td>. / 0.383</td>\n",
              "      <td>cat / 0.015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>can / 0.113</td>\n",
              "      <td>and / 0.001</td>\n",
              "      <td>rub / 0.000</td>\n",
              "      <td>and / 0.000</td>\n",
              "      <td>dog / 0.016</td>\n",
              "      <td>and / 0.000</td>\n",
              "      <td>and / 0.003</td>\n",
              "      <td>. / 0.006</td>\n",
              "      <td>a / 0.001</td>\n",
              "      <td>and / 0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>holds / 0.001</td>\n",
              "      <td>hold / 0.000</td>\n",
              "      <td>the / 0.000</td>\n",
              "      <td>cat / 0.000</td>\n",
              "      <td>cat / 0.003</td>\n",
              "      <td>. / 0.000</td>\n",
              "      <td>holds / 0.002</td>\n",
              "      <td>and / 0.006</td>\n",
              "      <td>the / 0.001</td>\n",
              "      <td>. / 0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dog / 0.000</td>\n",
              "      <td>dog / 0.000</td>\n",
              "      <td>a / 0.000</td>\n",
              "      <td>rub / 0.000</td>\n",
              "      <td>hold / 0.002</td>\n",
              "      <td>the / 0.000</td>\n",
              "      <td>hold / 0.001</td>\n",
              "      <td>a / 0.001</td>\n",
              "      <td>holds / 0.000</td>\n",
              "      <td>holds / 0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>cat / 0.000</td>\n",
              "      <td>rub / 0.000</td>\n",
              "      <td>and / 0.000</td>\n",
              "      <td>hold / 0.000</td>\n",
              "      <td>rub / 0.001</td>\n",
              "      <td>a / 0.000</td>\n",
              "      <td>rub / 0.001</td>\n",
              "      <td>the / 0.001</td>\n",
              "      <td>can / 0.000</td>\n",
              "      <td>the / 0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32cb46f9-8e08-4630-a878-811a748a534f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-32cb46f9-8e08-4630-a878-811a748a534f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-32cb46f9-8e08-4630-a878-811a748a534f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-276fb053-5da7-4bb0-b6d8-09072f89e082\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-276fb053-5da7-4bb0-b6d8-09072f89e082')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-276fb053-5da7-4bb0-b6d8-09072f89e082 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_prompt = train_dataset.tokenizer(\"She rubs\").to(trainer.device)\n",
        "generated_sequence,worldList,probability = trainer.model.generate1(encoded_prompt, trainer.device, temperature=0.8, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xRhcVAUPmnj0",
        "outputId": "a01fa335-ae17-4e38-d47b-166a8aeea501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'She rubs and holds the cat. dog . and dog.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "worldList0 = worldList[1:]\n",
        "probability0 = probability[1:]\n",
        "\n",
        "table = []\n",
        "\n",
        "for i in range(worldList0.shape[0]):\n",
        "  column = []\n",
        "  for t in range(worldList0.shape[1]):\n",
        "    column.append(train_dataset.tokenizer.decode(worldList0[i][t].reshape(1)) + f\" / {probability0[i][t]:.3f}\")\n",
        "  table.append(column)\n",
        "\n",
        "df1 = pd.DataFrame(data=table)"
      ],
      "metadata": {
        "id": "-Sd1dfgCmsGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "r-FHTFBDmxfS",
        "outputId": "67d03015-5205-483c-9ee0-d15a8546538c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                0               1             2              3              4  \\\n",
              "0     and / 0.346   holds / 0.994   the / 0.544    cat / 0.633      . / 0.931   \n",
              "1       a / 0.341     can / 0.002     a / 0.453    dog / 0.367      . / 0.067   \n",
              "2     the / 0.312     and / 0.001   and / 0.001    rub / 0.000    and / 0.001   \n",
              "3       . / 0.000       . / 0.001     s / 0.001      . / 0.000    rub / 0.000   \n",
              "4   holds / 0.000       s / 0.001   dog / 0.000   hold / 0.000   hold / 0.000   \n",
              "5     can / 0.000     dog / 0.000   cat / 0.000      . / 0.000    cat / 0.000   \n",
              "\n",
              "              5               6               7               8  \\\n",
              "0   dog / 0.582       . / 0.551     and / 0.775     dog / 0.950   \n",
              "1   cat / 0.417       . / 0.438       . / 0.197       I / 0.038   \n",
              "2     s / 0.000    hold / 0.004     dog / 0.016    hold / 0.006   \n",
              "3     . / 0.000       s / 0.002     the / 0.005     cat / 0.004   \n",
              "4   and / 0.000     and / 0.002       a / 0.005     rub / 0.001   \n",
              "5   rub / 0.000   holds / 0.001   holds / 0.001   holds / 0.001   \n",
              "\n",
              "                9  \n",
              "0       . / 0.613  \n",
              "1     and / 0.386  \n",
              "2       a / 0.001  \n",
              "3     the / 0.000  \n",
              "4   holds / 0.000  \n",
              "5     can / 0.000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6057feb8-47d7-49b9-9884-74e38c149d69\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>and / 0.346</td>\n",
              "      <td>holds / 0.994</td>\n",
              "      <td>the / 0.544</td>\n",
              "      <td>cat / 0.633</td>\n",
              "      <td>. / 0.931</td>\n",
              "      <td>dog / 0.582</td>\n",
              "      <td>. / 0.551</td>\n",
              "      <td>and / 0.775</td>\n",
              "      <td>dog / 0.950</td>\n",
              "      <td>. / 0.613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a / 0.341</td>\n",
              "      <td>can / 0.002</td>\n",
              "      <td>a / 0.453</td>\n",
              "      <td>dog / 0.367</td>\n",
              "      <td>. / 0.067</td>\n",
              "      <td>cat / 0.417</td>\n",
              "      <td>. / 0.438</td>\n",
              "      <td>. / 0.197</td>\n",
              "      <td>I / 0.038</td>\n",
              "      <td>and / 0.386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the / 0.312</td>\n",
              "      <td>and / 0.001</td>\n",
              "      <td>and / 0.001</td>\n",
              "      <td>rub / 0.000</td>\n",
              "      <td>and / 0.001</td>\n",
              "      <td>s / 0.000</td>\n",
              "      <td>hold / 0.004</td>\n",
              "      <td>dog / 0.016</td>\n",
              "      <td>hold / 0.006</td>\n",
              "      <td>a / 0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>. / 0.000</td>\n",
              "      <td>. / 0.001</td>\n",
              "      <td>s / 0.001</td>\n",
              "      <td>. / 0.000</td>\n",
              "      <td>rub / 0.000</td>\n",
              "      <td>. / 0.000</td>\n",
              "      <td>s / 0.002</td>\n",
              "      <td>the / 0.005</td>\n",
              "      <td>cat / 0.004</td>\n",
              "      <td>the / 0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>holds / 0.000</td>\n",
              "      <td>s / 0.001</td>\n",
              "      <td>dog / 0.000</td>\n",
              "      <td>hold / 0.000</td>\n",
              "      <td>hold / 0.000</td>\n",
              "      <td>and / 0.000</td>\n",
              "      <td>and / 0.002</td>\n",
              "      <td>a / 0.005</td>\n",
              "      <td>rub / 0.001</td>\n",
              "      <td>holds / 0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>can / 0.000</td>\n",
              "      <td>dog / 0.000</td>\n",
              "      <td>cat / 0.000</td>\n",
              "      <td>. / 0.000</td>\n",
              "      <td>cat / 0.000</td>\n",
              "      <td>rub / 0.000</td>\n",
              "      <td>holds / 0.001</td>\n",
              "      <td>holds / 0.001</td>\n",
              "      <td>holds / 0.001</td>\n",
              "      <td>can / 0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6057feb8-47d7-49b9-9884-74e38c149d69')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6057feb8-47d7-49b9-9884-74e38c149d69 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6057feb8-47d7-49b9-9884-74e38c149d69');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-23e3a797-3f5c-4a03-9d52-b5e4d3bf736d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-23e3a797-3f5c-4a03-9d52-b5e4d3bf736d')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-23e3a797-3f5c-4a03-9d52-b5e4d3bf736d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "section 3"
      ],
      "metadata": {
        "id": "7wWmq64tm3mL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb2_uLvoZgiK",
        "outputId": "6c5b9e7f-2991-4f32-b3d4-e6c2c9e4a21c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# The code below shows how to reload the model from the saved file; is useful things that take long to train\n",
        "model.load_state_dict(torch.load('model_large100K.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvU31habZgiK",
        "outputId": "90078a82-b1f9-4117-e728-1636f94d3d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'he believe of Mr. Wheeler a point of so intense that'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "# Example showing how the reloaded model still works\n",
        "encoded_prompt = train_dataset.tokenizer(\"he believe\").to(trainer.device)\n",
        "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example showing how the reloaded model still works\n",
        "encoded_prompt = train_dataset.tokenizer(\"in the end\").to(trainer.device)\n",
        "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CDv2Oy2Gp5Al",
        "outputId": "bb6d5cdc-13c9-43fc-d594-84fff4b54b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'in the end of the same5 had taken at a lever above'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example showing how the reloaded model still works\n",
        "encoded_prompt = train_dataset.tokenizer(\"coin\").to(trainer.device)\n",
        "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HYVagpAXqEMA",
        "outputId": "ca5bf47a-1390-494f-8efe-1c5174a95f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'coin there is a room of the one thousand of copper'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example showing how the reloaded model still works\n",
        "encoded_prompt = train_dataset.tokenizer(\"United States\").to(trainer.device)\n",
        "generated_sequence = trainer.model.generate(encoded_prompt, trainer.device, temperature=0.6, max_new_tokens=10)\n",
        "train_dataset.tokenizer.decode(generated_sequence[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C2Y0HM4cqRAF",
        "outputId": "f9d13648-a80f-467b-d9c9-50991eed991b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'United States Mint    was shield by the  '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6lLcjxrZgiK"
      },
      "outputs": [],
      "source": [
        "model.to(trainer.device)\n",
        "# store the saved model in a file, so can re-use later\n",
        "modelsavename= \"model_Large.pt\"  # change the name here to save in a specific file (and restore below)\n",
        "with open(modelsavename, \"wb\") as f:\n",
        "    torch.save(trainer.model.state_dict(), f)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "3c245645368b405f9e41f3dedb59d0df7c5d5feced548513488e8eb3fe8134cb"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}